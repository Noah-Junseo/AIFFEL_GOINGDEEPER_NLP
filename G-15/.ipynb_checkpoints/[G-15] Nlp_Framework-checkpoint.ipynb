{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "paperback-buddy",
   "metadata": {},
   "source": [
    "# NLP_GoingDeeper | 15. NLP Frameworkì˜ í™œìš©\n",
    "===\n",
    "\n",
    "ìµœì‹  NLP ê¸°ìˆ ë°œì „ì„ ì„ ë„í•˜ëŠ” ë‹¤ì–‘í•œ NLP Frameworkì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ê°€ì¥ ëŒ€í‘œì ì¸ Huggingface transformerë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ê³„êµ¬ì¡°ì™€ í™œìš©ë²•ì„ ê³µë¶€í•´ ë³¸ë‹¤.\n",
    "\n",
    "[ëª©ì°¨]\n",
    "- 15.2 ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„\n",
    "- 15.3 Huggingface transformers ê°œìš”\n",
    "- 15.4 Huggingface transformers (1) Model\n",
    "- 15.5 Huggingface transformers (2) Tokenizer\n",
    "- 15.6 Huggingface transformers (3) Processor\n",
    "- 15.7 Huggingface transformers (4) Config\n",
    "- 15.8 Huggingface transformers (5) Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-wallpaper",
   "metadata": {},
   "source": [
    "# 15-2. ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-manner",
   "metadata": {},
   "source": [
    "ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ì£¼ë¡œ Huggingface transformers Frameworkë¥¼ í†µí•´ NLP framworkê°€ ì–´ë–¤ ê²ƒì¸ì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Huggingface transformersì´ì „ì—ë„ NLP ë¶„ì•¼ì— ë„ë¦¬ ì•Œë ¤ì§„ frameworkë“¤ì´ ë§ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆë„ ë„ë¦¬ í™œìš©ë˜ê³  ìˆëŠ” ê²ƒë“¤ ìœ„ì£¼ë¡œ ì¢…ë¥˜ë³„ë¡œ ëª‡ ê°€ì§€ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- (ì°¸ê³ ) [Top NLP Libraries to Use 2020](https://towardsdatascience.com/top-nlp-libraries-to-use-2020-4f700cdb841f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-asthma",
   "metadata": {},
   "source": [
    "# 15-3. Huggingface transformers ê°œìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-dictionary",
   "metadata": {},
   "source": [
    "## Why Huggingface?\n",
    "\n",
    "(1) ê´‘ë²”ìœ„í•˜ê³  ì‹ ì†í•œ NLP ëª¨ë¸ ì§€ì›     \n",
    "(2) PyTorchì™€ Tensorflow ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥     \n",
    "(3) ì˜ ì„¤ê³„ëœ framework êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-doubt",
   "metadata": {},
   "source": [
    "## ì‹œì‘í•˜ê¸°\n",
    "\n",
    "huggingfaceì˜ Transformers ì„¤ì¹˜í•˜ëŠ” ë°©ë²•\n",
    "- pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9939cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚´ê°€ ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ì´ìœ ëŠ” ê° GPUí™˜ê²½ì— ë”°ë¥¸ ëª¨ë¸ ì†ë„ ë¹„êµ.\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0895f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\noah\\anaconda3\\envs\\mecab_aiffel\\lib\\site-packages (from sacremoses->transformers) (8.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "overall-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978194236755371}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-subscription",
   "metadata": {},
   "source": [
    "## Huggingface transformers ì„¤ê³„êµ¬ì¡° ê°œìš”\n",
    "\n",
    "NLP frameworkê°€ NLPëª¨ë¸ì„ í†µí•´ ì–´ë– í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì´ ì–´ë–»ê²Œ ì§„í–‰ë ì§€ ìƒê°í•´ ë´…ì‹œë‹¤.\n",
    "\n",
    "ë¨¼ì €, 1) Taskë¥¼ ì •ì˜í•˜ê³  ê·¸ì— ë§ê²Œ datasetì„ ê°€ê³µì‹œí‚µë‹ˆë‹¤. ê·¸ ì´í›„ 2) ì ë‹¹í•œ modelì„ ì„ íƒí•˜ê³  ì´ë¥¼ ë§Œë“­ë‹ˆë‹¤. 3)modelì— ë°ì´í„°ë“¤ì„ íƒœì›Œì„œ í•™ìŠµì„ ì‹œí‚¤ê³ , ì´ë¥¼ í†µí•´ ë‚˜ì˜¨ 4)weightì™€ ì„¤ì •(config)ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤. ì €ì¥í•œ modelì˜ checkpointëŠ” 5)ë°°í¬í•˜ê±°ë‚˜, evaluationì„ í•  ë•Œ ì‚¬ìš©í•˜ê³ ëŠ” í•˜ì£ .\n",
    "\n",
    "transformersëŠ” ìœ„ì™€ ê°™ì€ íë¦„ì— ë§ì¶”ì–´ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "taskë¥¼ ì •ì˜í•˜ê³  datasetì„ ì•Œë§ê²Œ ê°€ê³µí•˜ëŠ” Processors, í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” Tokenizer, ë‹¤ì–‘í•œ modelì„ ì •ì˜í•œ Model, optimizerì™€ í•™ìŠµ schedule(warm up ë“±)ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” Optimization, í•™ìŠµ ê³¼ì •ì„ ì „ë°˜ì„ ê´€ë¦¬í•˜ëŠ” Trainer, weightì™€ tokenizer, modelì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ê°ì¢… ì„¤ì •ì„ ì €ì¥í•˜ëŠ” Config ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-mexican",
   "metadata": {},
   "source": [
    "# 15-4. Huggingface transformers (1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-peoples",
   "metadata": {},
   "source": [
    "ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ë“¤ì€ PretrainedModel í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ê³  ìˆìŠµë‹ˆë‹¤. PretrainedModel í´ë˜ìŠ¤ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , ë‹¤ìš´ë¡œë“œí•˜ê³ , ì €ì¥í•˜ëŠ” ë“± ëª¨ë¸ ì „ë°˜ì— ê±¸ì³ ì ìš©ë˜ëŠ” ë©”ì†Œë“œë“¤ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒì† êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ì‹¤ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ì´ BERTì´ê±´, GPTì´ê±´ ìƒê´€ì—†ì´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ë‹¤ìš´ë¡œë“œ/ì €ì¥í•˜ëŠ” ë“±ì˜ ì‘ì—…ì— í™œìš©í•˜ëŠ” ë©”ì†Œë“œëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê²ƒì„ ë™ì¼í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì²« ë²ˆì§¸ë¡œëŠ” taskì— ì í•©í•œ ëª¨ë¸ì„ ì§ì ‘ ì„ íƒí•˜ì—¬ importí•˜ê³ , ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¡œë“œí•  ë•ŒëŠ” from_pretrainedë¼ëŠ” ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©°, Huggingfaceì˜ pretrained ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Huggingfaceì—ì„œ ì œê³µí•˜ëŠ” pretrained ëª¨ë¸ì´ë¼ë©´ ëª¨ë¸ì˜ ì´ë¦„ì„ stringìœ¼ë¡œ, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë¼ë©´ configì™€ ëª¨ë¸ì„ ì €ì¥í•œ ê²½ë¡œë¥¼ stringìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developing-newcastle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-modification",
   "metadata": {},
   "source": [
    "ë‘ ë²ˆì§¸ ë°©ë²•ì€, AutoModelì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì— ê´€í•œ ì •ë³´ë¥¼ ì²˜ìŒë¶€í„° ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë˜ì–´ ì¡°ê¸ˆ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floating-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-fifteen",
   "metadata": {},
   "source": [
    "ë°©ê¸ˆ ìœ„ì—ì„œ bert-base-casedë¼ê³  ì–¸ê¸‰ëœ ë¶€ë¶„ì´ ë³´ì´ì‹œë‚˜ìš”? ì´ê²ƒì€ Model IDì…ë‹ˆë‹¤. Huggingfaceê°€ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ pretrained modelì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒí• ì§€ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì´ IDë¥¼ í™œìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì–´ë–¤ ëª¨ë¸ì´ ì§€ì›ë˜ëŠ”ì§€ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [Pretrained models](https://huggingface.co/models)\n",
    "\n",
    "ê·¸ëŸ°ë°, ìœ„ì—ì„œ ì†Œê°œí•œ ë‘ ê°€ì§€ ë°©ë²•ì˜ ì°¨ì´ê°€ íŒŒì•…ë˜ì‹œë‚˜ìš”? ë¶ˆëŸ¬ì˜¤ê³ ì í•˜ëŠ” ëª¨ë¸ì˜ IDëŠ” bert-base-casedë¡œì„œ ë™ì¼í•©ë‹ˆë‹¤. ì‚¬ìš©ë²•ë„ ê±°ì˜ ë™ì¼í•œë°ìš”, ê²°ê³¼ì ìœ¼ë¡œ model. __ class __ ë¥¼ í™•ì¸í•´ ë³´ë©´ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ ë™ì¼í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ Pretrain, Downstream Task ë“± ìš©ë„ì— ë”°ë¼ ëª¨ë¸ì˜ Inputì´ë‚˜ Output shapeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AutoModelì„ í™œìš©í•œë‹¤ë©´ ëª¨ë¸ì˜ ìƒì„¸ì •ë³´ë¥¼ í™•ì¸í•  í•„ìš” ì—†ì´ Model IDë§Œìœ¼ë¡œë„ ì†ì‰½ê²Œ ëª¨ë¸ êµ¬ì„±ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì •í™•í•œ ìš©ë„ì— ë§ê²Œ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ë³„ ìƒì„¸ ì•ˆë‚´ í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì„œ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ BERTì˜ ìƒì„¸ í˜ì´ì§€ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [MODELS - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "\n",
    "ëª¨ë¸ë§ˆë‹¤ ê·¸ êµ¬ì¡°ëŠ” ë‹¤ë¥´ì§€ë§Œ ëŒ€ë¶€ë¶„ í•´ë‹¹ ëª¨ë¸ ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤(eg. TFBertModel)ê³¼ MainLayer class(eg. TFBertMainLayer)ì™€ Attention Class, Embedding Class ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹µë‹ˆë‹¤. ì¶”í›„ì— ëª¨ë¸ì´ ì–´ë–»ê²Œ ì§œì—¬ì¡ŒëŠ”ì§€ ë³´ì‹¤ ë•Œ __ init __ () ë©”ì†Œë“œ ì•ˆì— êµ¬ì„±ëœ ë¼ˆëŒ€ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ì„¸ìš” :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-raleigh",
   "metadata": {},
   "source": [
    "# 15-5. Huggingface transformers (2) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-australian",
   "metadata": {},
   "source": [
    "ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ë¬¸ì œë¥¼ í’€ ëª¨ë¸ì„ ì •í–ˆë‹¤ë©´, ì´ì œ ëª¨ë¸ì— ë„£ì„ inputì„ ë§Œë“¤ì–´ ì¤„ ì°¨ë¡€ì…ë‹ˆë‹¤.\n",
    "\n",
    "transformersëŠ” ë‹¤ì–‘í•œ tokenizerë¥¼ ê° ëª¨ë¸ì— ë§ì¶”ì–´ ì´ë¯¸ êµ¬ë¹„í•´ë‘ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í•  ì¼ì€ tokenizerë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒë¿ì´ëë‹ˆë‹¤. ì‚¬ìš©í•˜ê¸°ì— ì•ì„œì„œ, ë‚´ê°€ ì„ íƒí•œ modelì´ ì–´ë– í•œ tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì •ë„ ë¯¸ë¦¬ ì²´í¬í•´ë‘ëŠ” ì„¼ìŠ¤ëŠ” ëª¨ë‘ ì±™ê²¨ë‘ì…¨ê² ì£ ?:)\n",
    "\n",
    "Pretrained model ê¸°ë°˜ì˜ NLP frameworkë¥¼ ì‚¬ìš©í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ë‘ ê°€ì§€ í´ë˜ìŠ¤ëŠ” Modelê³¼ Tokenizerë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‘ ê°€ì§€ëŠ” ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° êµ¬ì¡°ê°€ ë™ì¼í•œ Modelì´ë¼ í•˜ë”ë¼ë„ Tokenizerê°€ ë‹¤ë¥´ê±°ë‚˜ Tokenizer ë‚´ì˜ Dictionaryê°€ ë‹¬ë¼ì§€ë©´ ì‚¬ì‹¤ìƒ ì™„ì „íˆ ë‹¤ë¥¸ ëª¨ë¸ì´ ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  TokenizerëŠ” ì–´ë–¤ ì–¸ì–´ë¥¼ ë‹¤ë£¨ëŠëƒ í•˜ëŠ” ì½”í¼ìŠ¤ ë°ì´í„°ì…‹ì— ë”°ë¼ì„œë„ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì „ ìŠ¤í…ì—ì„œ ì†Œê°œí–ˆë˜ Huggingfaceê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ ì¢…ë¥˜ ì¤‘ ëª‡ ê°œë§Œ ì˜ˆë¡œ ë“¤ì–´ë³¼ê¹Œìš”?\n",
    "\n",
    "- bert-base-uncased : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì„ ì—†ì•´ë‹¤(ì „ì²´ ì†Œë¬¸ìí™”)\n",
    "- bert-large-cased : BERT ëª¨ë¸ì¸ë°, 340MB íŒŒë¼ë¯¸í„°ì˜ ëŒ€í˜• ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì„ ìœ ì§€í–ˆë‹¤.\n",
    "- bert-base-multilingual-cased : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ë‹¤êµ­ì–´ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ìœ ì§€í–ˆë‹¤.\n",
    "\n",
    "tokenizer ë˜í•œ ì§ì ‘ ëª…ì‹œí•˜ì—¬ ë‚´ê°€ ì‚¬ìš©í•  ê²ƒì„ ì§€ì •í•´ ì£¼ê±°ë‚˜, AutoTokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ êµ¬ë¹„ëœ modelì— ì•Œë§ì€ tokenizerë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë•Œ ìœ ì˜í•  ì ì€, **modelì„ ì‚¬ìš©í•  ë•Œ ëª…ì‹œí–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ IDë¡œ tokenizerë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤**ëŠ” ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banned-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "normal-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-cartoon",
   "metadata": {},
   "source": [
    "ë¶ˆëŸ¬ì˜¨ tokenizerë¥¼ í•œ ë²ˆ ì‚¬ìš©í•´ë³¼ê¹Œìš”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "single-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-slide",
   "metadata": {},
   "source": [
    "ì´ ê²½ìš°ëŠ” BERTì˜ tokenizerì´ê¸° ë•Œë¬¸ì— ì¸ì½”ë”©ì´ ëœ input_ids ë¿ë§Œ ì•„ë‹ˆë¼, token_type_idsì™€ attention_maskê¹Œì§€ ëª¨ë‘ ìƒì„±ëœ input ê°ì²´ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tokenizerëŠ” batch ë‹¨ìœ„ë¡œ inputì„ ë°›ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entertaining-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-roller",
   "metadata": {},
   "source": [
    "ì´ ë°–ì—ë„ tokenizeí•  ë•Œì— padding, truncation ë“± ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì–´ë–¤ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€(Tensorflow ë˜ëŠ” PyTorch)ì— ë”°ë¼ input íƒ€ì…ì„ ë³€ê²½ ì‹œì¼œì£¼ëŠ” return_tensors ì¸ìë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corrected-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]])>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-liver",
   "metadata": {},
   "source": [
    "# 15-6. Huggingface transformers (3) Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-phoenix",
   "metadata": {},
   "source": [
    "ì§€ê¸ˆê¹Œì§€ ê°€ì¥ í•µì‹¬ì´ ë˜ëŠ” Model ë° Modelê³¼ ë°ì´í„°ì…‹ì„ ì—°ê²°í•´ ì£¼ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì—°ê²° ë„êµ¬ì¸ Tokenizerì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ°ë°, ê³¼ì—° Tokenizerê¹Œì§€ë§Œ ìˆìœ¼ë©´ ì–´ë–¤ Taskì´ë“  Modelì— ë„£ì„ ìˆ˜ ìˆëŠ” ì ì ˆí•œ ì…ë ¥ í˜•íƒœë¡œ ë³€ê²½í•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?\n",
    "\n",
    "ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ BERTì˜ pretrainingë§Œ í•˜ë”ë¼ë„, ëª¨ë¸ì— ë“¤ì–´ê°ˆ ì…ë ¥ ë¶€ë¶„ì„ êµ¬ì„±í•˜ë ¤ë©´\n",
    "\n",
    "- 1)ë‘ ê°œì˜ ë¬¸ì¥ì„ ê³¨ë¼ì„œ, Next Sentence Predictionì„ ìœ„í•´ ì ì ˆíˆ ë°°ì¹˜í•˜ê³ ,\n",
    "- 2)15%ì˜ ë§ˆìŠ¤í‚¹ í¬ì§€ì…˜ì„ ê³¨ë¼ë‚´ê¸° ìœ„í•œ ë³µì¡í•œ ê³¼ì •ì„ ê±°ì¹œ í›„, ì‹¬ì§€ì–´ ê·¸ ë§ˆí‚¹ ëŒ€ìƒë„ 10%, 10%ì˜ í™•ë¥ ë¡œ ë§ˆìŠ¤í‚¹ ëŒ€ì‹  ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ í•´ì£¼ì–´ì•¼ í•˜ê³ ,\n",
    "- 3)Next Sentence Predictionì„ ìœ„í•´ Segment Embeddingì„ ìœ„í•œ tensorë¥¼ ë”°ë¡œ ë§ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë¶€ë¶„ì˜ ê³¼ì •ì´ ì˜ì™¸ë¡œ (ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒë§Œí¼ì´ë‚˜) ë³µì¡í•œ ê³¼ì •ì„ì„ ê²½í—˜í•´ ë³´ì…¨ê² ì£ ?\n",
    "\n",
    "ì´ë ‡ê²Œ ë³µì¡í•œ ê³¼ì •ì„ ìœ„í•´ì„œëŠ” Tokenizerë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ Huggingfaceì—ì„œëŠ” Processorë¼ëŠ” ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” Processor ì¤‘ Sequence Classification íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤ì¸ DataProcessorì˜ ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"sequence classificationì„ ìœ„í•´ dataë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë³¸ processor\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        tensor dictì—ì„œ exampleì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì†Œë“œ\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"train dataì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"dev data(validation data)ì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"test dataì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"data setì— ì‚¬ìš©ë˜ëŠ” ë¼ë²¨ë“¤ì„ ë¦¬í„´í•˜ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        tfds(tensorflow-datasets)ì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ DataProcessorì— ì•Œë§ê²Œ ê°€ê³µí•´ì£¼ëŠ” ë©”ì†Œë“œ\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"tabìœ¼ë¡œ êµ¬ë¶„ëœ .tsvíŒŒì¼ì„ ì½ì–´ë“¤ì´ëŠ” í´ë˜ìŠ¤ ë©”ì†Œë“œ\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-convert",
   "metadata": {},
   "source": [
    "processorëŠ” raw dataë¥¼ ê°€ê³µí•˜ì—¬ modelì— íƒœìš¸ ìˆ˜ ìˆëŠ” í˜•íƒœë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ì‘ì—…ì„ í•´ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "Hugging faceëŠ” SQuAD, GLUE ë“± ê°€ì¥ ëŒ€í‘œì ì¸ NLPì˜ ë¬¸ì œë“¤ì— ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ processorë¥¼ ë§Œë“¤ì–´ ë‘ì—ˆìŠµë‹ˆë‹¤. ë§Œì•½, ë‚´ê°€ ì§ì ‘ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ê³  ì‹¶ë‹¤ë©´, ë‚´ ë°ì´í„°ì— ì•Œë§ì€ processorë¥¼ ì§ì ‘ ì •ì˜í•´ì•¼ê² ì£ ?\n",
    "\n",
    "Task ë³„ ë³µì¡í•œ ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” processorë¥¼ ì§ì ‘ ë§Œë“œì‹¤ ë•ŒëŠ”, DataProcessorë¥¼ ìƒì†ë°›ì•„ì„œ ë§Œë“¤ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ì— ë”°ë¼ì„œ ì¶”ê°€í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ìƒê¸¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í”„ë¡œì íŠ¸ ë…¸ë“œì—ì„œëŠ” processorë¥¼ ìƒì†ë°›ì•„ í™œìš©í•˜ëŠ” ë¶€ë¶„ë„ ì‹¤ìŠµì„ í†µí•´ ì§„í–‰í•´ ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "- â— ì£¼ì˜    \n",
    "    raise NotImplementedError()ëŠ” ì§€ì›Œì£¼ì‹œëŠ” ê²ƒ ìŠì§€ ë§ˆì„¸ìš”! ì¶”ìƒ í´ë˜ìŠ¤ì—ì„œ ê¼­ êµ¬í˜„í•´ì•¼ í•  ë¶€ë¶„ì´ NotImplementedë¡œ ë‚¨ì•„ìˆë‹¤ë©´ ì—¬ëŸ¬ë¶„ì´ ì§  ProcessorëŠ” Tokenizerì™€ í˜‘ë ¥í•˜ì—¬ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-leonard",
   "metadata": {},
   "source": [
    "# 15-7. Huggingface transformers (4) Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-journalism",
   "metadata": {},
   "source": [
    "configëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ìš”ì†Œë“¤ì„ ëª…ì‹œí•œ jsoníŒŒì¼ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ jsoníŒŒì¼ì—ëŠ” batch size, learning rate, weight_decayë“± trainì— í•„ìš”í•œ ìš”ì†Œë“¤ë¶€í„° tokenizerì— íŠ¹ìˆ˜ í† í°(special token eg.[MASK])ë“¤ì„ ë¯¸ë¦¬ ì„¤ì •í•˜ëŠ” ë“± ì„¤ì •ì— ê´€í•œ ì „ë°˜ì ì¸ ê²ƒë“¤ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "PretrainedModelì„ save_pretrained ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ë©´ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ì €ì¥ë˜ë„ë¡ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "hugging faceì˜ pretrained modelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìë™ìœ¼ë¡œ configíŒŒì¼ì´ ë¡œë“œë˜ì–´ ëª…ì‹œí•  í•„ìš”ê°€ ì—†ì§€ë§Œ, ì„¤ì •ì„ ë³€ê²½í•˜ê³  ì‹¶ê±°ë‚˜ ë‚˜ë§Œì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œì—ëŠ” configíŒŒì¼ì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "config ë˜í•œ model, tokenizerì²˜ëŸ¼ Model IDë§Œ ìˆìœ¼ë©´, Config í´ë˜ìŠ¤ë¥¼ ëª…í™•íˆ ì§€ì •í•˜ê±°ë‚˜ í˜¹ì€ AutoConfigë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "meaningful-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "union-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-consortium",
   "metadata": {},
   "source": [
    "ë‘ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ configì˜ ë‚´ìš©ì— ë³„ë‹¤ë¥¸ ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ëª¨ë¸ì„ ì´ë¯¸ ìƒì„±í–ˆë‹¤ë©´ model.configìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eligible-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "config = model.config\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-edmonton",
   "metadata": {},
   "source": [
    "# 15-8. Huggingface transformers (5) Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-indicator",
   "metadata": {},
   "source": [
    "trainerëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. training, fine-tuning, evaluation ëª¨ë‘ trainer classë¥¼ ì´ìš©í•˜ì—¬ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tensorflowì˜ ê²½ìš° tf.keras.model APIë¥¼ ì´ìš©í•˜ì—¬ì„œë„ Huggingfaceë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ í™œìš©í•´ í•™ìŠµì´ë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë™ì•ˆ ë§ì´ í™œìš©í•´ ë³´ì•˜ë˜ model.fit()ì´ë‚˜ model.predict()ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ë§Œ, TFTrainerë¥¼ ì´ìš©í•  ê²½ìš°ì—ëŠ” TrainingArguments ë¥¼ í†µí•´ Huggingface í”„ë ˆì„ì›Œí¬ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ë“¤ì„ í†µí•©ì ìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” Huggingfaceë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ tf.keras.model APIë¥¼ ì´ìš©í•´ í™œìš©í•˜ëŠ” ê²½ìš°ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sonic-pottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Results=====\n",
      "TFBertForPreTrainingOutput(loss=None, prediction_logits=array([[[ -7.402364 ,  -7.3620872,  -7.4495645, ...,  -6.195387 ,\n",
      "          -5.8944345,  -6.366632 ],\n",
      "        [ -7.8288083,  -8.057953 ,  -7.864195 , ...,  -6.4195423,\n",
      "          -6.302309 ,  -6.7620687],\n",
      "        [-11.548872 , -11.551803 , -11.483164 , ...,  -8.114498 ,\n",
      "          -8.313562 ,  -9.444115 ],\n",
      "        ...,\n",
      "        [ -3.2659318,  -3.7415216,  -2.5793612, ...,  -4.01182  ,\n",
      "          -2.4980402,  -3.0753539],\n",
      "        [-12.233541 , -12.029903 , -11.799584 , ...,  -8.84179  ,\n",
      "          -9.094511 , -10.500351 ],\n",
      "        [-10.636517 , -11.070655 , -11.032907 , ...,  -8.146264 ,\n",
      "          -9.582066 , -10.668421 ]]], dtype=float32), seq_relationship_logits=array([[ 1.6308241 , -0.71683615]], dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForPreTraining, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "sentence = \"Hello, This is test for bert TFmodel.\"\n",
    "\n",
    "input_ids = tf.constant(tokenizer.encode(sentence, add_special_tokens=True))[None, :]  # Batch size 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "pred = model.predict(input_ids)\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-israeli",
   "metadata": {},
   "source": [
    "TFTrainerë¥¼ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” í•™ìŠµì— í•„ìš”í•œ argumentsì„ TFTrainingArgumentsì„ í†µí•´ì„œ ì •ì˜í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” TFTrainerë¥¼ ì‚¬ìš©í•˜ì—¬ Huggingface ëª¨ë¸ì˜ í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ ëª¨ë¸ì´ êµ¬ë™ë˜ì–´ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•œ í”„ë¡œì íŠ¸ êµ¬ì„±ì€ ë‹¤ìŒ í”„ë¡œì íŠ¸ ë…¸ë“œì—ì„œ ë‹¤ë£¨ê² ì§€ë§Œ, ì´ë²ˆ ë…¸ë“œì—ì„œ ì‚´í´ë³¸ Model, Tokenizer ë° ë°ì´í„°ì…‹ êµ¬ì„±ì´ TFTrainingArgumentsë¥¼ í†µí•´ì„œ TFTrainerì— ì–´ë–»ê²Œ ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acoustic-possible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5fa624e9b54799a32512d572f0d98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d03f35a4284c0d8196a8cd2648824e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821e238dac1940469e36ac1a8a07bbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7d6a6c75204bae9a86a81ea3274d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict, Optional\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    glue_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "# TFTrainingArguments ì •ì˜\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',              # outputì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    num_train_epochs=1,              # train ì‹œí‚¬ ì´ epochs\n",
    "    per_device_train_batch_size=16,  # ê° device ë‹¹ batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation ì‹œì— batch size\n",
    "    warmup_steps=500,                # learning rate schedulerì— ë”°ë¥¸ warmup_step ì„¤ì •\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # logê°€ ì €ì¥ë  ê²½ë¡œ\n",
    "    do_train=True,                        # train ìˆ˜í–‰ì—¬ë¶€\n",
    "    do_eval=True,                        # eval ìˆ˜í–‰ì—¬ë¶€\n",
    "    eval_steps=1000\n",
    ")\n",
    "\n",
    "# model, tokenizer ìƒì„±\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "with training_args.strategy.scope():    # training_argsê°€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” modelì˜ ë²”ìœ„ë¥¼ ì§€ì •\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_pt=bool(\".bin\" in model_name_or_path),\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-antibody",
   "metadata": {},
   "source": [
    "ìœ„ì™€ ê°™ì´ Huggingface í”„ë ˆì„ì›Œí¬ êµ¬ì¡°ì— ë”°ë¼ Modelê³¼ Tokenizerë¥¼ ê°„ë‹¨íˆ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ëŠ” ê¸°ì¡´ì— ì‚´í´ë³¸ ê²ƒê³¼ í° ì°¨ì´ëŠ” ì—†ì§€ë§Œ model ìƒì„± ì‹œì— training_argsì˜ scope ì•ˆì—ì„œ ì§„í–‰í–ˆë‹¤ëŠ” ê²ƒì´ ëˆˆì— ë•ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ TFTrainer ì‚¬ìš© ì‹œ ê²°ì •ì ìœ¼ë¡œ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ with êµ¬ë¬¸ì„ ìƒëµí•˜ë©´ TFTrainerì— ì „ë‹¬í•˜ê³ í”ˆ ì˜µì…˜ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•„ ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì´ ì˜¤ë™ì‘í•˜ê²Œ ë˜ëŠ” ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "innovative-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Noah\\tensorflow_datasets\\glue\\mrpc\\2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd9311d132c49d699017833269e4e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e02dc2e46140f1aaf36e8ab0038e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Noah\\tensorflow_datasets\\glue\\mrpc\\2.0.0.incomplete6XF678\\glue-train.tfrecord*...:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Noah\\tensorflow_datasets\\glue\\mrpc\\2.0.0.incomplete6XF678\\glue-validation.tfrecord*...:   0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Noah\\tensorflow_datasets\\glue\\mrpc\\2.0.0.incomplete6XF678\\glue-test.tfrecord*...:   0%|    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to C:\\Users\\Noah\\tensorflow_datasets\\glue\\mrpc\\2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\anaconda3\\envs\\mecab_Aiffel\\lib\\site-packages\\transformers\\data\\processors\\glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
      "C:\\Users\\Noah\\anaconda3\\envs\\mecab_Aiffel\\lib\\site-packages\\transformers\\data\\processors\\glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "C:\\Users\\Noah\\anaconda3\\envs\\mecab_Aiffel\\lib\\site-packages\\transformers\\trainer_tf.py:108: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459945113570602}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "ds, info = tfds.load('glue/mrpc', with_info=True)\n",
    "train_dataset = glue_convert_examples_to_features(ds['train'], tokenizer, 128, 'mrpc')\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['train'].num_examples))\n",
    "\n",
    "# TFTrainer ìƒì„±\n",
    "trainer = TFTrainer(\n",
    "    model=model,                          # í•™ìŠµì‹œí‚¬ model\n",
    "    args=training_args,                  # TFTrainingArgumentsì„ í†µí•´ ì„¤ì •í•œ arguments\n",
    "    train_dataset=train_dataset,   # training dataset\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ì§„í–‰\n",
    "trainer.train()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_dataset = glue_convert_examples_to_features(ds['test'], tokenizer, 128, 'mrpc')\n",
    "test_dataset = test_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['test'].num_examples))\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "infrared-wireless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì‘ì—… ì†Œìš” ì‹œê°„ì€ ì•½ 185ì´ˆì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "work_time = round(end_time - start_time)\n",
    "print(f'ì´ ì‘ì—… ì†Œìš” ì‹œê°„ì€ ì•½ {work_time}ì´ˆì…ë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-badge",
   "metadata": {},
   "source": [
    "ì´í›„ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì—¬, model, training_argsê³¼ í•¨ê»˜ TFTrainerì— ì „ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ê°€ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤. ì´í›„ trainer.train()ì„ í˜¸ì¶œí•˜ë©´ ì‹¤ì œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
