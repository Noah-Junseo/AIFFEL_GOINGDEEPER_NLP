{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "taken-assignment",
   "metadata": {},
   "source": [
    "# NLP_GoingDeeper | 16. HuggingFace 커스텀 프로젝트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-abuse",
   "metadata": {},
   "source": [
    "NLP framework에서 가장 대표적인 Huggingface transformers를 활용하여 자신만의 커스텀 프로젝트를 만들어 보도록 할 것입니다.\n",
    "\n",
    "다만, **Huggingface transformers의 구조 분석만으로는 실제로 활용 가능한 자신만의 무기가 되지 못합니다.**\n",
    "\n",
    "이번 노드에서는 실전 프로젝트를 가정하고 Huggingface transformers framework를 활용하여 빠르게 자신만의 커스텀 프로젝트를 구성해 보는 실습을 진행하게 됩니다. framework 내의 Model, Tokenizer, Processor 등이 어떻게 활용되는지 꼼꼼히 살펴보고, 하나의 framework에 익숙해진다면 다른 framework에 적응하는 것도 훨씬 수월해질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-extraction",
   "metadata": {},
   "source": [
    "## 1. GLUE Benchmark Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-grove",
   "metadata": {},
   "source": [
    "Pretrained model의 성능을 측정하기 위해 최근은 SQuAD 등 기존에 유명한 데이터셋 한 가지만 가지고 성능을 논하는 것이 아니라, classification, summarization, reasoning, Q&A 등 NLP 모델의 성능을 평가할 수 있는 다양한 task를 해당 모델 하나만을 이용해 모두 수행해 보면서 종합적인 성능을 논하는 것이 일반화되었습니다.\n",
    "\n",
    "그중 NLP 모델의 성능을 측정하기 위한 데이터셋으로 최근 활용되는 대표적인 것 중에 **General Language Understanding Evaluation(GLUE) benchmark Dataset이 있습니다. 총 10가지 데이터셋이 있습니다.** 각각의 개요는 다음과 같습니다.\n",
    "\n",
    "관련 링크는 아래에 있습니다.\n",
    "\n",
    "**아래 10가지의 task에 대하여 : https://gluebenchmark.com/tasks**\n",
    "\n",
    "**리더보드 : https://gluebenchmark.com/leaderboard**\n",
    "\n",
    "- CoLA : 문법에 맞는 문장인지 판단\n",
    "- MNLI : 두 문장의 관계 판단(entailment, contradiction, neutral)\n",
    "- MNLI-MM : 두 문장이 안 맞는지 판단\n",
    "- MRPC : 두 문장의 유사도 평가\n",
    "- SST-2 : 감정분석\n",
    "- STS-B : 두 문장의 유사도 평가\n",
    "- QQP : 두 질문의 유사도 평가\n",
    "- QNLI : 질문과 paragraph 내 한 문장이 함의 관계(entailment)인지 판단\n",
    "- RTE : 두 문장의 관계 판단(entailment, not_entailment)\n",
    "- WNLI : 원문장과 대명사로 치환한 문장 사이의 함의 관계 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-cholesterol",
   "metadata": {},
   "source": [
    "최근에는 한 가지 task에만 최적화된 모델이 아니라, **다양한 형태의 문제를 골고루 잘 푸는 모델**을 찾기 위한 노력이 계속되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-egypt",
   "metadata": {},
   "source": [
    "## 2. Huggingface가 제공하는 GLUE task 예제 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-classic",
   "metadata": {},
   "source": [
    "**Huggingface와 같은 NLP framework는 해당 framework를 활용하여 새로 만들어진 모델의 성능을 빠르게 평가해 볼 수 있도록 하는 예제 코드를 제공하고 있습니다.** 이런 예제 코드가 없다면 모델을 새로 만들 때마다 그 성능을 비교 측정해 보기 위한 작업이 너무나 번거롭게 될 것입니다.\n",
    "\n",
    "우선, Huggingface의 예제 코드를 들여다보는 것으로부터 시작 하겠습니다.\n",
    "\n",
    "**아래 명령어를 Cloud Shell에서 입력해 설치를 반드시 수행해야 이후의 진행에 문제가 없으니, 꼭 설치해야 합니다! 👍**"
   ]
  },
  {
   "attachments": {
    "002.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "herbal-haiti",
   "metadata": {},
   "source": [
    "![002.PNG](attachment:002.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-sewing",
   "metadata": {},
   "source": [
    "설치가 완료되었다면 아래 코드를 터미널에서 수행해 보겠습니다."
   ]
  },
  {
   "attachments": {
    "003.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "floral-personality",
   "metadata": {},
   "source": [
    "![003.PNG](attachment:003.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-appreciation",
   "metadata": {},
   "source": [
    "수행 결과는 아래와 같습니다."
   ]
  },
  {
   "attachments": {
    "001.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "detailed-network",
   "metadata": {},
   "source": [
    "![001.PNG](attachment:001.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-enterprise",
   "metadata": {},
   "source": [
    "위 코드는 10가지 GLUE task 중 'mrpc' task를 수행하는 예제 코드입니다.\n",
    "\n",
    "이 코드는 **Huggingface의 framework 기반으로 BERT bert-base-cased을 활용하여 'mrpc' task를 수행**합니다.\n",
    "\n",
    "만약 task_name 및 다른 파라미터를 적절히 변경한 후 수행하면 다른 GLUE task도 간단히 수행해 볼 수 있을 것입니다.\n",
    "\n",
    "model도 다양하게 바꾸어 보면서 수월하게 수행 가능할 것입니다.\n",
    "\n",
    "이 예제만으로도 NLP framework의 강력함을 손쉽게 느껴볼 수 있을 것입니다.\n",
    "\n",
    "다만, 우리는 NLP framework를 활용해서 그저 GLUE task만 수행하지는 않을 것입니다. **이 framework를 활용하여 해결해야 할 실무 프로젝트를 빠르게 수행하기 위한 노드이기 때문입니다.**\n",
    "\n",
    "그런 상황을 가정해서, 방금 수행해 본 GLUE 'mrpc' task를 나만의 커스텀 프로젝트로 구성해서 다시 해결해 보도록 합니다. 이 과정을 통해 Huggingface framework에 대해 좀 더 명확하게 이해하실 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-plasma",
   "metadata": {},
   "source": [
    "## 3. 커스텀 프로젝트 제작 : Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-homeless",
   "metadata": {},
   "source": [
    "### mrpc 데이터셋 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-winner",
   "metadata": {},
   "source": [
    "본격적으로 Huggingface framework를 활용하기 위해, 프로젝트를 수행하기 위한 첫 단계인 데이터 분석에 들어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9939cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내가 시간을 측정하는 이유는 각 GPU환경에 따른 모델 속도 비교.\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tutorial-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 로드 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoConfig\n",
    "from dataclasses import asdict\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "\n",
    "print(\"라이브러리 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-capital",
   "metadata": {},
   "source": [
    "GLUE 데이터셋은 홈페이지에서 원본을 다운로드할 수도 있지만, 이번에는 **tensorflow_datasets에서 제공**하는 것을 이용해 보겠습니다.\n",
    "\n",
    "아래 코드를 수행해 보면 3668개의 훈련 데이터셋이 존재함을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "happy-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /aiffel/tensorflow_datasets/glue/mrpc/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/aiffel/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /aiffel/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, info = tfds.load('glue/mrpc', with_info=True)\n",
    "info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-basket",
   "metadata": {},
   "source": [
    "**data는 tf.data.Dataset을 상속받은 클래스의 형태**가 될 것입니다. 우선 1개의 데이터만 가져다가 어떻게 생겼는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chicken-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-inspector",
   "metadata": {},
   "source": [
    "데이터셋 안에 어떤 항목이 정의되어 있는지 확인할 수 있었습니다. 실제 내용도 한번 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unknown-spread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n",
      "tf.Tensor(b'The rovers act as robotic geologists , moving on six wheels .', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "examples = data['train'].take(1)\n",
    "for example in examples:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['label']\n",
    "    print(sentence1)\n",
    "    print(sentence2)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-search",
   "metadata": {},
   "source": [
    "### Processor의 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-politics",
   "metadata": {},
   "source": [
    "Huggingface transformers에서 task별로 데이터셋을 가공하는 일반적인 클래스 구조인 Processor를 활용합니다.\n",
    "\n",
    "아래는 추상 클래스인 Processor를 한번 상속받은 후, Sequence Classification task를 수행하는 모델의 Processor 추상 클래스인 DataProcessor입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "realistic-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        Gets an example from a dict with tensorflow tensors.\n",
    "\n",
    "        Args:\n",
    "            tensor_dict: Keys and values should match the corresponding Glue\n",
    "                tensorflow_dataset examples.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
    "        examples to the correct format.\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-point",
   "metadata": {},
   "source": [
    "아직은 추상클래스 상태이기 때문에 그대로 사용하면 NotImplementedError를 발생시키는 메소드들이 포함되어 있습니다. 이 메소드들을 오버라이드해야 실제 사용 가능한 클래스가 얻어지게 됩니다.\n",
    "\n",
    "아래는 'mrpc' 원본 데이터셋을 처리하여 모델에 입력할 수 있도록 정리해 주는 **MrpcProcessor 클래스입니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-underground",
   "metadata": {},
   "source": [
    "이것만으로는 클래스 구조와 메커니즘이 눈에 잘 안 들어올 것입니다.\n",
    "\n",
    "여기서 우선 주목해야 할 메소드는 **get_example_from_tensor_dict()** 입니다. 실제로 이 메소드가 어떤 역할을 하게 되는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recognized-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------원본데이터------\n",
      "{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>, 'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>}\n",
      "------processor 가공데이터------\n",
      "InputExample(guid=1680, text_a='The identical rovers will act as robotic geologists , searching for evidence of past water .', text_b='The rovers act as robotic geologists , moving on six wheels .', label='0')\n"
     ]
    }
   ],
   "source": [
    "processor = MrpcProcessor()\n",
    "examples = data['train'].take(1)\n",
    "\n",
    "for example in examples:\n",
    "    print('------원본데이터------')\n",
    "    print(example)  \n",
    "    example = processor.get_example_from_tensor_dict(example)\n",
    "    print('------processor 가공데이터------')\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-college",
   "metadata": {},
   "source": [
    "원본과 비교해보니 Processor가 하는 역할이 무엇인지 좀 더 명확해졌을지 모르겠습니다. 😅\n",
    "\n",
    "한마디로 요약하자면 Processor는 'Raw Dataset를 Annotated Dataset으로 변환'하는 역할을 합니다. 항목별로 text_a, text_b, label 등의 annotation이 포함된 InputExample로 변환되어 있음을 알 수 있습니다.\n",
    "\n",
    "다음 코드는 **tfds_map()** 메소드를 활용한 경우입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raising-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputExample(guid=1680, text_a='The identical rovers will act as robotic geologists , searching for evidence of past water .', text_b='The rovers act as robotic geologists , moving on six wheels .', label='0')\n"
     ]
    }
   ],
   "source": [
    "examples = (data['train'].take(1))\n",
    "for example in examples:\n",
    "    example = processor.get_example_from_tensor_dict(example)\n",
    "    example = processor.tfds_map(example)\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-movie",
   "metadata": {},
   "source": [
    "이전과 별다른 차이는 없습니다. tfds_map는 label을 가공하는 메소드인데, 이미 숫자로 잘 가공된 label이기 때문에 특별한 변화가 없습니다.\n",
    "\n",
    "실제 label을 확인하여 Binary Classification 문제로 잘 정의되고 있는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "actual-cancellation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = processor.get_labels()\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "professional-jersey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-cement",
   "metadata": {},
   "source": [
    "## 4. 커스텀 프로젝트 제작 : Tokenizer, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-slave",
   "metadata": {},
   "source": [
    "Processor를 통해 Framework을 활용하여 데이터셋을 가공하는 작업을 잘 진행했다면 이미 절반 이상 진행했다고 보아도 됩니다. NLP 모델링의 핵심을 이루는 Tokenizer와 Model은 framework에서 이미 잘 만들어져 있는 것을 쉽게 가져다 쓸 수 있기 때문입니다.\n",
    "\n",
    "그럼 이전 스텝에서 만든 MRPCProcessor 클래스와 framework를 결합시켜 나가는 과정을 진행해 보겠습니다. 우선 아래와 같이 tokenizer와 model을 간단히 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thermal-armstrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-mouse",
   "metadata": {},
   "source": [
    "이제 processor와 tokenizer, 원본 데이터셋을 결합하여 model에 입력할 데이터셋을 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bottom-tomato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"claasification\") :\n",
    "    if max_length is None :\n",
    "        max_length = tokenizer.max_len\n",
    "    if label_list is None:\n",
    "        label_list = processor.get_labels()\n",
    "        print(\"Using label list %s\" % (label_list))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    labels = [label_map[example.label] for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        print(\"*** Example ***\")\n",
    "        print(\"guid: %s\" % (example.guid))\n",
    "        print(\"features: %s\" % features[i])\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legendary-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"classification\") :\n",
    "    \"\"\"\n",
    "    :param examples: tf.data.Dataset\n",
    "    :param tokenizer: pretrained tokenizer\n",
    "    :param max_length: example의 최대 길이(기본값 : tokenizer의 max_len)\n",
    "    :param task: GLUE task 이름\n",
    "    :param label_list: 라벨 리스트\n",
    "    :param output_mode: \"regression\" or \"classification\"\n",
    "\n",
    "    :return: task에 맞도록 feature가 구성된 tf.data.Dataset\n",
    "    \"\"\"\n",
    "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
    "    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)\n",
    "    label_type = tf.int64\n",
    "\n",
    "    def gen():\n",
    "        for ex in features:\n",
    "            d = {k: v for k, v in asdict(ex).items() if v is not None}\n",
    "            label = d.pop(\"label\")\n",
    "            yield (d, label)\n",
    "\n",
    "    input_names = [\"input_ids\"] + tokenizer.model_input_names\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({k: tf.int32 for k in input_names}, label_type),\n",
    "        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
    "    )\n",
    "\n",
    "print(\"함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-atlantic",
   "metadata": {},
   "source": [
    "**_glue_convert_examples_to_features()** 함수는 processor가 생성한 example을 tokenizer로 인코딩하여 feature로 변환하는 역할을 합니다. 이후 **tf_glue_convert_examples_to_features()** 함수는 내부적으로 _glue_convert_examples_to_features()를 호출해서 얻은 feature를 바탕으로 tf.data.Dataset을 생성하여 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "twelve-taxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1']\n",
      "*** Example ***\n",
      "guid: 1680\n",
      "features: InputFeatures(input_ids=[101, 1996, 7235, 9819, 2097, 2552, 2004, 20478, 21334, 2015, 1010, 6575, 2005, 3350, 1997, 2627, 2300, 1012, 102, 1996, 9819, 2552, 2004, 20478, 21334, 2015, 1010, 3048, 2006, 2416, 7787, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 1456\n",
      "features: InputFeatures(input_ids=[101, 2625, 2084, 2322, 3867, 1997, 23193, 1005, 1055, 4341, 2052, 2272, 2013, 2437, 13891, 1998, 3259, 2044, 1996, 2436, 17848, 5309, 2003, 2949, 1012, 102, 2625, 2084, 2322, 3867, 1997, 23193, 1005, 1055, 4341, 2052, 2272, 2013, 2437, 13891, 1998, 3259, 2044, 1996, 2436, 17848, 5309, 2003, 3143, 1010, 10262, 2216, 5661, 4995, 1005, 1056, 2853, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 3017\n",
      "features: InputFeatures(input_ids=[101, 6804, 1011, 2158, 14177, 1002, 12457, 1012, 1021, 2454, 1999, 2049, 2834, 2197, 2095, 1998, 2253, 2006, 2000, 5425, 1002, 28203, 1012, 1021, 2454, 1012, 102, 6804, 1011, 2158, 1010, 6758, 18720, 1011, 2410, 1010, 14177, 1002, 12457, 1012, 1021, 2454, 1999, 2049, 2034, 5353, 1998, 2253, 2006, 2000, 2202, 1999, 1002, 28203, 1012, 1021, 2454, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 2896\n",
      "features: InputFeatures(input_ids=[101, 1996, 2526, 2117, 4284, 3463, 2123, 1005, 1056, 2421, 4481, 2013, 2256, 2814, 2012, 4012, 4502, 4160, 1012, 102, 1996, 2095, 1011, 3283, 3616, 2079, 2025, 2421, 4481, 2013, 4012, 4502, 4160, 3274, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 499\n",
      "features: InputFeatures(input_ids=[101, 9168, 1019, 1012, 1019, 2003, 2800, 3322, 1999, 1996, 2142, 2163, 1998, 2710, 1010, 2005, 1037, 3225, 3976, 1997, 2055, 1002, 2260, 1010, 6352, 1012, 102, 9168, 1019, 1012, 1019, 2003, 2085, 2800, 1999, 1996, 1057, 1012, 1055, 1012, 1998, 2710, 2083, 7513, 2449, 7300, 24501, 24038, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-sector",
   "metadata": {},
   "source": [
    "tf_glue_convert_examples_to_features() 함수가 최종적으로 모델에 전달될 tf.data.Dataset 인스턴스를 생성합니다. 위 코드는 그렇게 생성된 학습 단계 데이터셋 train_dataset입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([  101,  1996,  7235,  9819,  2097,  2552,  2004, 20478, 21334,\n",
      "        2015,  1010,  6575,  2005,  3350,  1997,  2627,  2300,  1012,\n",
      "         102,  1996,  9819,  2552,  2004, 20478, 21334,  2015,  1010,\n",
      "        3048,  2006,  2416,  7787,  1012,   102,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "examples = train_dataset.take(1)\n",
    "for example in examples:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-burden",
   "metadata": {},
   "source": [
    "그럼 전체 데이터셋을 크게 3가지로 나누어 구성해 보겠습니다. **train, validaion, test** 세 가지로 나누어 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "constant-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1']\n",
      "*** Example ***\n",
      "guid: 1680\n",
      "features: InputFeatures(input_ids=[101, 1996, 7235, 9819, 2097, 2552, 2004, 20478, 21334, 2015, 1010, 6575, 2005, 3350, 1997, 2627, 2300, 1012, 102, 1996, 9819, 2552, 2004, 20478, 21334, 2015, 1010, 3048, 2006, 2416, 7787, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 1456\n",
      "features: InputFeatures(input_ids=[101, 2625, 2084, 2322, 3867, 1997, 23193, 1005, 1055, 4341, 2052, 2272, 2013, 2437, 13891, 1998, 3259, 2044, 1996, 2436, 17848, 5309, 2003, 2949, 1012, 102, 2625, 2084, 2322, 3867, 1997, 23193, 1005, 1055, 4341, 2052, 2272, 2013, 2437, 13891, 1998, 3259, 2044, 1996, 2436, 17848, 5309, 2003, 3143, 1010, 10262, 2216, 5661, 4995, 1005, 1056, 2853, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 3017\n",
      "features: InputFeatures(input_ids=[101, 6804, 1011, 2158, 14177, 1002, 12457, 1012, 1021, 2454, 1999, 2049, 2834, 2197, 2095, 1998, 2253, 2006, 2000, 5425, 1002, 28203, 1012, 1021, 2454, 1012, 102, 6804, 1011, 2158, 1010, 6758, 18720, 1011, 2410, 1010, 14177, 1002, 12457, 1012, 1021, 2454, 1999, 2049, 2034, 5353, 1998, 2253, 2006, 2000, 2202, 1999, 1002, 28203, 1012, 1021, 2454, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 2896\n",
      "features: InputFeatures(input_ids=[101, 1996, 2526, 2117, 4284, 3463, 2123, 1005, 1056, 2421, 4481, 2013, 2256, 2814, 2012, 4012, 4502, 4160, 1012, 102, 1996, 2095, 1011, 3283, 3616, 2079, 2025, 2421, 4481, 2013, 4012, 4502, 4160, 3274, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 499\n",
      "features: InputFeatures(input_ids=[101, 9168, 1019, 1012, 1019, 2003, 2800, 3322, 1999, 1996, 2142, 2163, 1998, 2710, 1010, 2005, 1037, 3225, 3976, 1997, 2055, 1002, 2260, 1010, 6352, 1012, 102, 9168, 1019, 1012, 1019, 2003, 2085, 2800, 1999, 1996, 1057, 1012, 1055, 1012, 1998, 2710, 2083, 7513, 2449, 7300, 24501, 24038, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n"
     ]
    }
   ],
   "source": [
    "# train 데이터셋\n",
    "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)\n",
    "train_dataset_batch = train_dataset.shuffle(100).batch(16).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "roman-asian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1']\n",
      "*** Example ***\n",
      "guid: 3155\n",
      "features: InputFeatures(input_ids=[101, 1996, 2265, 1005, 1055, 8503, 5360, 2353, 1011, 4284, 16565, 2566, 3745, 2011, 1037, 10647, 1012, 102, 1996, 2194, 2056, 2023, 19209, 16565, 2011, 1037, 10647, 1037, 3745, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 2472\n",
      "features: InputFeatures(input_ids=[101, 26568, 8040, 12995, 6767, 1010, 4464, 1010, 9601, 1996, 7709, 2012, 1996, 9925, 3016, 2181, 29277, 2073, 2016, 2038, 2042, 2542, 2005, 2195, 2086, 1010, 2056, 2014, 2269, 1010, 3960, 8040, 10606, 21222, 1012, 102, 1996, 7270, 2001, 3718, 9317, 2013, 26568, 8040, 12995, 6767, 1010, 4464, 1010, 2012, 1996, 9925, 3016, 1011, 2181, 29277, 2073, 2016, 2038, 2973, 2005, 2195, 2086, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 3584\n",
      "features: InputFeatures(input_ids=[101, 1996, 2817, 1010, 2405, 6928, 1999, 1996, 3485, 8382, 4167, 2470, 1010, 2003, 3497, 2000, 2036, 6611, 2000, 4286, 1010, 2049, 6048, 2056, 1012, 102, 1996, 2817, 1010, 4146, 2006, 1996, 14332, 1997, 4975, 12328, 1010, 2001, 2108, 2405, 2651, 1999, 1996, 3485, 8382, 4167, 2470, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 3523\n",
      "features: InputFeatures(input_ids=[101, 2009, 2036, 4107, 1037, 2328, 1011, 1999, 16660, 2094, 5956, 9573, 7170, 2121, 2061, 2008, 2152, 1011, 4304, 16660, 2094, 5956, 3638, 2064, 2022, 2109, 2302, 2383, 2000, 16500, 2019, 3176, 2490, 9090, 1012, 102, 1996, 1055, 2509, 2278, 18827, 12740, 2038, 1037, 2328, 1011, 1999, 16660, 2094, 5956, 9573, 7170, 2121, 1010, 2005, 2742, 1010, 2061, 2008, 2152, 1011, 4304, 16660, 2094, 5956, 3638, 2064, 2022, 5361, 2302, 2019, 3176, 2490, 9090, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 1782\n",
      "features: InputFeatures(input_ids=[101, 2852, 2928, 23680, 20898, 1010, 5655, 1005, 1055, 2155, 3460, 1010, 2056, 2065, 1996, 4319, 2018, 2042, 8564, 3041, 5655, 2052, 2031, 6025, 2062, 1997, 2010, 4167, 4972, 1012, 102, 2852, 2928, 23680, 20898, 1010, 1996, 2155, 1005, 1055, 14246, 1010, 2056, 2018, 1996, 4319, 2042, 8564, 2000, 5655, 3041, 1010, 2002, 2052, 2031, 6025, 2062, 1997, 2010, 4167, 3853, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n"
     ]
    }
   ],
   "source": [
    "# validation 데이터셋\n",
    "validation_dataset = tf_glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, processor=processor)\n",
    "validation_dataset_batch = validation_dataset.shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moving-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1']\n",
      "*** Example ***\n",
      "guid: 163\n",
      "features: InputFeatures(input_ids=[101, 6661, 1999, 8670, 2020, 2091, 1015, 1012, 1019, 3867, 2012, 16923, 7279, 3401, 2011, 16087, 2692, 13938, 2102, 1010, 2125, 1037, 2659, 1997, 17943, 2361, 1010, 1999, 1037, 3621, 6428, 3452, 2414, 3006, 1012, 102, 6661, 1999, 8670, 2020, 2091, 2093, 3867, 2012, 13913, 1011, 1015, 1013, 1018, 7279, 3401, 2011, 5641, 22394, 13938, 2102, 1010, 2125, 1037, 2659, 1997, 17943, 7279, 3401, 1010, 1999, 1037, 6428, 3006, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 131\n",
      "features: InputFeatures(input_ids=[101, 1996, 2148, 4759, 5237, 1998, 13116, 3757, 2036, 2056, 2009, 2052, 5466, 2041, 2030, 4604, 2067, 2035, 3010, 12486, 2747, 1999, 3573, 1012, 102, 1996, 2148, 4759, 5237, 1998, 13116, 3757, 2056, 2009, 2052, 15121, 2030, 2709, 2035, 3010, 12486, 1999, 3573, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 1579\n",
      "features: InputFeatures(input_ids=[101, 1000, 2047, 19095, 2015, 2134, 1005, 1056, 9979, 2122, 3197, 2066, 2027, 2071, 2031, 1010, 1000, 2056, 5487, 4830, 2271, 1010, 3472, 1997, 1996, 3222, 1012, 102, 1000, 2047, 19095, 2015, 2134, 1005, 1056, 9979, 2122, 3197, 2066, 2027, 2071, 2031, 1010, 1000, 5487, 1059, 1012, 4830, 2271, 1010, 1996, 3222, 1005, 1055, 3472, 1010, 2056, 7483, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 1151\n",
      "features: InputFeatures(input_ids=[101, 1000, 1045, 2428, 4669, 2032, 1998, 1045, 2145, 2079, 1010, 1000, 9946, 2632, 2239, 2409, 1996, 9536, 7483, 1012, 102, 1998, 1045, 2428, 4669, 2032, 1010, 1998, 1045, 2145, 2079, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 1021\n",
      "features: InputFeatures(input_ids=[101, 4389, 2865, 7711, 1998, 1996, 6556, 3295, 1997, 1996, 2642, 2352, 2073, 1996, 3554, 3631, 2041, 2081, 2009, 5263, 2000, 12210, 2054, 3047, 1012, 102, 4389, 2865, 7711, 1998, 1996, 6556, 3295, 1997, 1996, 13249, 2081, 2009, 5263, 2000, 12210, 2054, 3047, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n"
     ]
    }
   ],
   "source": [
    "# test 데이터셋\n",
    "test_dataset = tf_glue_convert_examples_to_features(data['test'], tokenizer, max_length=128, processor=processor)\n",
    "test_dataset_batch = test_dataset.shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-completion",
   "metadata": {},
   "source": [
    "## 5. 커스텀 프로젝트 제작 : Train/Evaluation, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-gibson",
   "metadata": {},
   "source": [
    "**데이터셋과 모델이 모두 준비된 상태임을 가정**한 스텝입니다. 이후의 과정은 아래와 같이 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-cement",
   "metadata": {},
   "source": [
    "### tf.keras.model 을 활용한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-width",
   "metadata": {},
   "source": [
    "우선, 우리에게 익숙한 **model.fit()** 을 이용한 모델 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "connected-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(processor.get_labels())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complicated-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-drunk",
   "metadata": {},
   "source": [
    "학습을 진행합니다. 이 학습은 이미 잘 훈련된 BERT 모델을 가져다가 fine-tuning하는 작업입니다.\n",
    "\n",
    "편의상 2 Epoch만 진행한 후, 그 성능을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "competent-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fadceb588a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fadceb588a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fadceb588a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - ETA: 0s - loss: 0.6047 - acc: 0.6941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 113s 839ms/step - loss: 0.6045 - acc: 0.6942 - val_loss: 0.4972 - val_acc: 0.7868\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 96s 831ms/step - loss: 0.4673 - acc: 0.7859 - val_loss: 0.3560 - val_acc: 0.8456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac101c6890>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝에서 배치처리를 진행한 데이터셋(xxxx_dataset_batch)을 활용\n",
    "model.fit(train_dataset_batch, epochs=2, steps_per_epoch=115, \n",
    "                validation_data=validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-diamond",
   "metadata": {},
   "source": [
    "학습이 잘 진행되었다면 아래와 같이 테스트 결과를 만들어 확인해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "connected-population",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 29s 265ms/step - loss: 0.4946 - acc: 0.7774\n",
      "[0.49460989236831665, 0.7773913145065308]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_dataset_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alleged-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n",
      "Loss = 0.494610\tAccuracy = 0.777391\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    for i, v in enumerate(result) :\n",
    "        if i == 0 :\n",
    "            writer.write(\"Loss = %f\\t\" %(v))\n",
    "        if i == 1 :\n",
    "            writer.write(\"Accuracy = %f\\n\" %(v))\n",
    "print(\"완료!\")\n",
    "\n",
    "#파일에 쓴 테스트 결과 확인\n",
    "!cat ~/aiffel/transformers/eval_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-graham",
   "metadata": {},
   "source": [
    "### TFTrainer 를 활용한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-minutes",
   "metadata": {},
   "source": [
    "이번에는 **Huggingface의 TFTrainer를 활용해** 학습을 진행해 봅시다.\n",
    "\n",
    "이전 노드에서 살펴본 것처럼 TFTrainer를 활용하기 위해서는 TFTrainingArguments에 학습 관련 설정을 미리 지정해 두어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "included-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# TFTrainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    "    glue_compute_metrics,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_output_modes,\n",
    "    glue_processors,\n",
    "    glue_tasks_num_labels,\n",
    ")\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=output_dir,            # output이 저장될 경로\n",
    "    num_train_epochs=3,              # train 시킬 총 epochs\n",
    "    per_device_train_batch_size=16,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation 시에 batch size\n",
    "    warmup_steps=500,                # learning rate scheduler에 따른 warmup_step 설정\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # log가 저장될 경로\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000\n",
    ")\n",
    "\n",
    "max_seq_length=128\n",
    "task_name = \"mrpc\"\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-upgrade",
   "metadata": {},
   "source": [
    "아래에서 생성하게 될 TFTrainer의 인자로 넘겨주어야 할 것 중에 **compute_metrics** 메소드가 있습니다.\n",
    "\n",
    "이것은 task가 classification인지 regression인지에 따라 모델의 출력 형태가 달라지므로 task 별로 적합한 출력 형식을 고려해 모델의 성능을 계산하는 방법을 미리 지정해 두는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "labeled-egypt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    if output_mode == \"classification\":\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "    elif output_mode == \"regression\":\n",
    "        preds = np.squeeze(p.predictions)\n",
    "    return glue_compute_metrics(task_name, preds, p.label_ids)\n",
    "\n",
    "output_mode = glue_output_modes[task_name]\n",
    "output_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-snake",
   "metadata": {},
   "source": [
    "TFTrainer를 활용할 때 데이터셋에서 잊지 않아야 할 것이 **tf.data.experimental.assert_cardinality() 를 데이터셋에 적용**해 주는 것입니다.\n",
    "\n",
    "이를 호출해 주지 않으면 TFTrainer.train() 에서 assert fail이 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pediatric-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무사합니다.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['train'].num_examples))\n",
    "validation_dataset = validation_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['validation'].num_examples))\n",
    "test_dataset = test_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['test'].num_examples))\n",
    "\n",
    "print(\"무사합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-timing",
   "metadata": {},
   "source": [
    "이제 TFTrainer를 생성해서 본격적으로 학습을 시작해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "previous-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TFTrainer(\n",
    "    model=model,                           # 학습시킬 model\n",
    "    args=training_args,                  # TFTrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_dataset,    # training dataset\n",
    "    eval_dataset=validation_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "renewable-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-rugby",
   "metadata": {},
   "source": [
    "학습이 끝나면 Evaluation을 진행하여 위 model.fit() 으로 학습한 경우와 비교해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "broke-glossary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/metrics/__init__.py:66: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/metrics/__init__.py:42: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss = 0.5594404765537807\r\n",
      "eval_acc = 0.822265625\r\n",
      "eval_f1 = 0.8765264586160109\r\n",
      "eval_acc_and_f1 = 0.8493960418080054\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    result = trainer.evaluate()\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results2.txt\")\n",
    "\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        for key, value in result.items():\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "        results.update(result)\n",
    "        \n",
    "#파일에 쓴 테스트 결과 확인\n",
    "!cat ~/aiffel/transformers/eval_results2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "infrared-wireless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 작업 소요 시간은 약 185초입니다.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "work_time = round(end_time - start_time)\n",
    "print(f'총 작업 소요 시간은 약 {work_time}초입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662e752",
   "metadata": {},
   "source": [
    "## 16-6. 프로젝트 : 커스텀 프로젝트 직접 만들기\n",
    "실습 코드에서 수행해 본 내용을 토대로, 이번에는 GLUE dataset의 mnli task를 수행하는 프로젝트를 커스텀 프로젝트 형태로 진행해 봅시다.\n",
    "\n",
    "`mnli` task는 이전 스텝에서 사용한 BERT를 사용하면 학습이 제대로 되지 않습니다. [여기](https://huggingface.co/models)를 참조하여 BERT가 아닌 다른 모델을 선택하세요. \n",
    "\n",
    "tensorflow와 해당 모델에 대한 task로 검색하면 사용할 수 있는 모델이 나옵니다. \n",
    "\n",
    "그 후 선택한 모델의 _tokenizer_와 해당 모델에 대한 task 와 모델 의 정보를 [여기](https://huggingface.co/docs/transformers/index)에서 찾아 여러분의 프로젝트를 완성해 보세요.\n",
    "\n",
    "그냥 run_glue.py를 돌려보는 방식으로 진행하는 것을 원하는 것은 아닙니다. 아래와 같은 순서를 지켜서 진행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1bd483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "1.22.3\n",
      "4.18.0\n",
      "1.1\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 버전을 확인해 봅니다. 사용할 라이브러리 버전을 둘러봅시다.\n",
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import argparse\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(argparse.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed531025",
   "metadata": {},
   "source": [
    "## STEP 1. mnli 데이터셋을 분석해 보기\n",
    "`tensorflow-datasets`를 이용하여 `glue/mnli`를 다운로드하려면 `tensorflow-datasets` 라이브러리 버전을 올려야 합니다.\n",
    "\n",
    "pip install tensorflow-datasets -U\n",
    "\n",
    "위 명령어를 통해 라이브러리 업그레이드를 진행해 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e31ea",
   "metadata": {},
   "source": [
    "## STEP 2. MNLIProcessor클래스 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a6b07",
   "metadata": {},
   "source": [
    "## STEP 3. 위에서 구현한 processor 및 Huggingface에서 제공하는 tokenizer를 활용하여 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8c983",
   "metadata": {},
   "source": [
    "## STEP 4. model을 생성하여 학습 및 테스트를 진행해 보기\n",
    "\n",
    "💡 힌트\n",
    "\n",
    "- 혹시 STEP 2의 진행에 어려움을 겪고 계신다면 transformer 프로젝트 내부를 살펴보시면 참고할만한 예시 코드를 찾아볼 수 있을 것입니다. \n",
    "- transformers의 공식 github을 참고하는 것도 좋은 방법이에요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
