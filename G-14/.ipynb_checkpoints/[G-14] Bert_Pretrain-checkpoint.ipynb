{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjustable-uncertainty",
   "metadata": {},
   "source": [
    "# NLP_GoingDeeper | 14. BERT pretrained model 제작\n",
    "===\n",
    "\n",
    "가장 대표적인 pretrained ㅣlanguage model인 BERT의 pretrain 전과정을 진행해 보면서 BERT의 핵심원리를 깊이 이해해 본다.\n",
    "\n",
    "오늘은 일반적인 10M 정도의 작은 파라미터 사이즈의 BERT 모델을 만들어, 수백 MB 수준의 코퍼스 기반으로 pretrain 을 진행해 보도록 하겠습니다. 하지만 진행되는 과정은 정식 BERT와 동일할 테니 이를 토대로 pretrained model이 어떻게 만들어지는지를 경험해 봅니다. 모델을 만들고 학습시키는 것 이상으로 코퍼스 데이터를 가공해서 학습시켜야 할 task에 적합한 형태의 데이터셋으로 만들어가는 것이 큰 비중을 차지한다는 것을 알게 될 것입니다.\n",
    "    \n",
    "[목차]\n",
    "- 들어가며\n",
    "- 14.2 Tokenizer 준비\n",
    "- 14.3 데이터 전처리 (1) MASK 생성\n",
    "- 14.4 데이터 전처리 (2) NSP pair 생성\n",
    "- 14.5 데이터 전처리 (3) 데이터셋 완성\n",
    "- 14.6 BERT 모델 구현\n",
    "- 14.7 pretrain 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-circulation",
   "metadata": {},
   "source": [
    "# 14-2. Tokenizer 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-navigation",
   "metadata": {},
   "source": [
    "BERT등의 pretrained model이 나오게 되었을 즈음 자연어처리 분야의 또 다른 중요한 흐름 중 하나는 BPE 등의 subword 기반의 토크나이징 기법이 주요한 방법론으로 굳어졌다는 점입니다. GPT의 BPE, BERT의 WordPiece 모델 등의 성공이 더욱 사람들에게 subword 기반의 토크나이저에 대한 확신을 주었습니다.\n",
    "\n",
    "오늘 우리는 SentencePiece 기반의 토크나이저를 준비하는 것으로 BERT pretrain 과정을 시작할 것입니다. 이 과정 자체는 이미 익숙하실 것이라 생각합니다.\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9939cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내가 시간을 측정하는 이유는 각 GPU환경에 따른 모델 속도 비교.\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-press",
   "metadata": {},
   "source": [
    "준비해 둔 한글 나무위키 코퍼스로부터 32000의 vocab_size를 갖는 sentencepiece 모델을 생성해 보겠습니다.\n",
    "\n",
    "BERT에 사용되는 [MASK], [SEP], [CLS] 등의 주요 특수문자가 vocab에 포함되어야 함에 주의해 주세요. 아래와 같이 모델을 생성하게 되면 약 30분 정도가 소요될 것입니다. 오래 기다리는 것이 힘드신 분은 미리 클라우드에 저장된 파일을 사용하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruled-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# import os\n",
    "# corpus_file = 'C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/data/kowiki.txt'\n",
    "# prefix = 'ko_32000'\n",
    "# vocab_size = 32000\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "#     \" --model_type=bpe\" +\n",
    "#     \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "#     \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "#     \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "#     \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "#     \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "#     \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-encyclopedia",
   "metadata": {},
   "source": [
    "sentencepiece 모델 학습이 끝난 후 생성된 ko_32000.model, ko_32000.vocab 두 파일은 커널이 생성되었을 때의 홈디렉토리에 생성되었을 것입니다. 이 두 파일을 현재 진행하는 디렉토리 아래로 이동시킨 후 계속 진행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sophisticated-message",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/data'\n",
    "model_dir = 'C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-frontier",
   "metadata": {},
   "source": [
    "토크나이저가 잘 만들어졌는지 확인해 봅시다. 어떤 토큰이 만들어졌는지, 토크나이징 결과가 어떻게 나오는지 살펴볼까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "typical-instrument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frequent-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-pendant",
   "metadata": {},
   "source": [
    "토크나이저가 잘 작동하나요? 방금 우리는 SentencePiece 모델을 이용해 간단한 BERT의 Masked Language Model 학습용 데이터를 하나 생성해 보았습니다.\n",
    "\n",
    "다음 절부터 본격적으로 데이터 전처리 과정에 돌입하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-composition",
   "metadata": {},
   "source": [
    "# 14-3. 데이터 전처리 (1) MASK 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-village",
   "metadata": {},
   "source": [
    "BERT의 Masked Language Model은 GPT의 Next Token Prediction 태스크처럼 다음이 이어질 단어는? 을 맞추는 게 아니라 마스킹 된 다음 빈칸에 알맞은 단어는? 문제를 푸는 형식으로 구성됩니다. 이런 빈칸은 전체 토큰의 15% 정도가 적당하다고 합니다.\n",
    "\n",
    "이전 스텝의 Masked LM 데이터셋 예시에서 출발해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exact-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-advocacy",
   "metadata": {},
   "source": [
    "15%를 마스킹 한다고 해도 생각해 볼 것이 더 있습니다. Subword 기반으로 토크나이징을 했을 때 \\_대, [MASK], 민국이라고 가운데를 마스킹 했을 경우 해당 [MASK]가 '한'일 거라는 건 너무 쉽게 맞출 수 있습니다. '대한민국'이라는 패턴을 아주 자주 보게 될 테니까요.\n",
    "\n",
    "그래서 Masked LM 태스크를 구성할 땐 띄어쓰기 단위로 한꺼번에 마스킹해 주는 것이 좋습니다. 다음과 같이 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "selective-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] ['▁추적', '추', '적']\n",
      "[4] ['▁비가']\n",
      "[5] ['▁내리는']\n",
      "[6, 7, 8] ['▁날', '이었', '어']\n",
      "[9, 10] ['▁그날', '은']\n",
      "[11, 12, 13] ['▁', '왠', '지']\n",
      "[14, 15] ['▁손', '님이']\n",
      "[16] ['▁많아']\n",
      "[17] ['▁첫']\n",
      "[18] ['▁번에']\n",
      "[19, 20] ['▁삼', '십']\n",
      "[21] ['▁전']\n",
      "[22, 23] ['▁둘째', '번']\n",
      "[24, 25] ['▁오', '십']\n",
      "[26] ['▁전']\n",
      "[27, 28] ['▁오랜', '만에']\n",
      "[29, 30] ['▁받아', '보는']\n",
      "[31] ['▁십']\n",
      "[32, 33] ['▁전', '짜리']\n",
      "[34, 35, 36] ['▁백', '통', '화']\n",
      "[37, 38, 39] ['▁서', '푼', '에']\n",
      "[41] ['▁손바닥']\n",
      "[42, 43] ['▁위', '엔']\n",
      "[44, 45] ['▁기쁨', '의']\n",
      "[46, 47] ['▁눈', '물이']\n",
      "[48] ['▁흘러']\n",
      "[49, 50, 51] ['▁컬', '컬', '한']\n",
      "[52] ['▁목에']\n",
      "[53, 54] ['▁모', '주']\n",
      "[55, 56, 57] ['▁한', '잔', '을']\n",
      "[58, 59] ['▁적', '셔']\n",
      "[60] ['▁몇']\n",
      "[61] ['▁달']\n",
      "[62] ['▁포']\n",
      "[63] ['▁전부터']\n",
      "[64, 65, 66] ['▁콜', '록', '거리는']\n",
      "[67] ['▁아내']\n",
      "[68] ['▁생각에']\n",
      "[69, 70] ['▁그', '토록']\n",
      "[71] ['▁먹고']\n",
      "[72, 73] ['▁싶다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask 하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-diana",
   "metadata": {},
   "source": [
    "- [startswith(str)](https://wikidocs.net/137643) : 지정한 문자열로 시작하면 True, 그렇지 않다면 False를 반환        \n",
    "- u\"\\u2581\" : ▁(토큰에 사용하는 두꺼운 언더스코어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "portuguese-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18],\n",
       " [44, 45],\n",
       " [24, 25],\n",
       " [49, 50, 51],\n",
       " [31],\n",
       " [63],\n",
       " [41],\n",
       " [52],\n",
       " [22, 23],\n",
       " [71],\n",
       " [17],\n",
       " [19, 20],\n",
       " [60],\n",
       " [32, 33],\n",
       " [62],\n",
       " [46, 47],\n",
       " [29, 30],\n",
       " [72, 73],\n",
       " [6, 7, 8],\n",
       " [64, 65, 66],\n",
       " [67],\n",
       " [9, 10],\n",
       " [26],\n",
       " [55, 56, 57],\n",
       " [61],\n",
       " [34, 35, 36],\n",
       " [37, 38, 39],\n",
       " [21],\n",
       " [58, 59],\n",
       " [48],\n",
       " [69, 70],\n",
       " [4],\n",
       " [27, 28],\n",
       " [42, 43],\n",
       " [14, 15],\n",
       " [68],\n",
       " [5],\n",
       " [11, 12, 13],\n",
       " [1, 2, 3],\n",
       " [16],\n",
       " [53, 54]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-denial",
   "metadata": {},
   "source": [
    "개선된 Masking 로직을 다음과 같이 구현해 보았습니다. 마스킹 된 결과를 이전과 비교해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "still-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁그레이트', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0..1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-medium",
   "metadata": {},
   "source": [
    "Masked LM의 라벨 데이터도 아래와 같이 생성하여 정리해 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "happy-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-flash",
   "metadata": {},
   "source": [
    "## 🔶 create_pretrain_mask() : Masked LM을 위한 코퍼스 생성 메소드\n",
    "\n",
    "이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "solar-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-grade",
   "metadata": {},
   "source": [
    "create_pretrain_mask() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "filled-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '[MASK]', '[MASK]', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '[MASK]', '끊', '▁규장', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '[MASK]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [16, 17, 37, 38, 39, 48, 52, 53, 54, 67]\n",
      "mask_label : ['▁많아', '▁첫', '▁서', '푼', '에', '▁흘러', '▁목에', '▁모', '주', '▁아내']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-broadway",
   "metadata": {},
   "source": [
    "# 14-4. 데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-hands",
   "metadata": {},
   "source": [
    "BERT의 pretrain task로 Next Sentence Prediction이 있습니다. 문장 2개를 붙여 놓고 두 문장이 이어지는 것인지 아닌지 문장 호응관계를 맞출 수 있게 하는 것입니다.\n",
    "\n",
    "아래 문장을 예시로 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "restricted-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foreign-division",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'],\n",
       " ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'],\n",
       " ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-circus",
   "metadata": {},
   "source": [
    "우선 원문에서 이어진 두 문장씩 짝지어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "animal-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "separated-murder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "tokens_a: 16 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아']\n",
      "tokens_b: 50 ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "tokens_a: 35 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가']\n",
      "tokens_b: 30 ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "tokens_a: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens_b: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-steps",
   "metadata": {},
   "source": [
    "짝지은 두 문장을 그대로 두면 NSP task의 true label 케이스가 되고, 둘의 순서를 뒤바꾸면 false label 케이스가 되겠죠? 두 문장의 최대 길이를 유지하도록 trim을 적용한 후 50%의 확률로 true/false 케이스를 생성해 보겠습니다.\n",
    "\n",
    "- trim : 컴퓨터 프로그래밍에서 트리밍 또는 스트리핑은 문자열에서 선행 및 후행 공백을 제거하는 문자열 조작입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "retired-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hundred-evanescence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 42 ['▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러']\n",
      "tokens_b: 19 ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 0\n",
      "tokens_a: 37 ['만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "tokens_b: 24 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 32 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-wrestling",
   "metadata": {},
   "source": [
    "이제 두 문장 사이에 segment 처리를 해주어야 합니다. 첫 번째 문장의 segment는 모두 0으로, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주는 것으로 마무리됩니다.\n",
    "\n",
    "이전 스텝의 create_pretrain_mask()까지 함께 호출되어 Mask LM용 데이터셋과 NSP용 데이터셋이 결합된 하나의 데이터셋으로 완성될 것입니다. BERT의 pretrain 은 두 가지 task가 동시에 수행되니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ranking-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 8 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어']\n",
      "tokens_b: 53 ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포']\n",
      "tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]']\n",
      "masked index: 9 [17, 19, 24, 27, 52, 55, 56, 57, 60]\n",
      "masked label: 9 ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 1\n",
      "tokens_a: 16 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어']\n",
      "tokens_b: 45 ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간']\n",
      "tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]']\n",
      "masked index: 9 [11, 12, 13, 24, 25, 31, 52, 53, 61]\n",
      "masked label: 9 ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 44 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 6 [4, 18, 28, 36, 37, 40]\n",
      "masked label: 6 ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        # tokens & aegment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "endangered-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [17, 19, 24, 27, 52, 55, 56, 57, 60], 'mask_label': ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']}\n",
      "{'tokens': ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 13, 24, 25, 31, 52, 53, 61], 'mask_label': ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 18, 28, 36, 37, 40], 'mask_label': ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-webcam",
   "metadata": {},
   "source": [
    "## 🔶 create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드\n",
    "\n",
    "이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alleged-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-illustration",
   "metadata": {},
   "source": [
    "create_pretrain_instances() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complex-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁날', '이었', '어', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '[MASK]', '[MASK]', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '[MASK]', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '[MASK]', '[MASK]', '[MASK]', '▁적', '셔', '▁몇', '부에는', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 14, 15, 36, 50, 51, 52, 56], 'mask_label': ['▁그날', '은', '▁삼', '십', '▁손바닥', '▁한', '잔', '을', '▁달']}\n",
      "{'tokens': ['[CLS]', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '[MASK]', '[MASK]', '▁일찍', '이라도', '▁들어와', '껍', '▁무역을', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와', '[SEP]', '▁생각에', '▁그', '토록', '[MASK]', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '[MASK]', '▁살', '▁수', '꾹', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 30, 31, 42, 49, 50, 51, 54], 'mask_label': ['▁싶', '으면', '▁달라', '던', '▁먹고', '▁그릇', '을', '▁이제는', '▁있어']}\n",
      "{'tokens': ['[CLS]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '[MASK]', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '[MASK]', '[MASK]', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '나는', '▁세기', '01', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 19, 20, 40, 41, 42], 'mask_label': ['▁점점', '▁걱', '정은', '▁좋', '을', '까']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-steering",
   "metadata": {},
   "source": [
    "# 14-5. 데이터 전처리 (3) 데이터셋 완성\n",
    "\n",
    "이제 우리가 다루어야 할 kowiki.txt에 대해 본격적으로 들여다보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fixed-programming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = 'C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r' , encoding='utf8') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-employer",
   "metadata": {},
   "source": [
    "전체 라인 수가 확인되시나요? 거의 400만 개에 육박하는 수치입니다.\n",
    "\n",
    "위키 문서는 하나의 도큐먼트가 주제 키워드에 대해 상세 내용이 설명으로 따라붙어 있는 형태로 구성되어 있지요? 도큐먼트 주제별로 잘 나눠지는지도 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hired-ecuador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2430401011df45829353501d0dec30d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지미', '▁카터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인권', '과', '▁중재', '▁역할에', '▁대한', '▁공로를', '▁인정받아', '▁노벨', '▁평화', '상을', '▁받게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념을', '▁다루는', '▁학문이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용해서', '▁공', '리로', '▁구성된', '▁추상', '적', '▁구조를', '▁연구하는', '▁학문', '으로', '▁여겨', '지기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학문', '들과', '▁깊은', '▁연', '관을', '▁맺고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학의', '▁분야', '들과는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁있다는', '▁차이가', '▁있다고', '▁한다', '.', '▁수', '학자들은', '▁그러한', '▁개념', '들에', '▁대해서', '▁추측', '을', '▁하고', ',', '▁적절', '하게', '▁선택', '된', '▁정의', '와', '▁공리', '로부터의', '▁엄', '밀한', '▁연', '역을', '▁통해서', '▁추측', '들의', '▁진', '위를', '▁파악', '한다', '.']\n",
      "['▁수', '학의', '▁기초를', '▁확실히', '▁세우', '기', '▁위해', ',', '▁수리', '논', '리', '학과', '▁집합', '론이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범주', '론이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위기', '”', '라는', '▁말은', '▁대략', '▁1900', '년에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날에도', '▁계속되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논쟁', '에', '▁의해', '▁촉발', '되었으며', ',', '▁그', '▁논쟁', '에는', '▁칸', '토', '어의', '▁집합', '론과', '▁브라우', '어', '-', '힐', '베르트', '▁논쟁이', '▁포함되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상수']\n",
      "['▁수학에서', '▁상수', '란', '▁그', '▁값이', '▁변하지', '▁않는', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '과는', '▁상관없이', '▁정의된다', '.']\n",
      "['▁특정', '▁수학', '▁상수', ',', '▁예를', '▁들면', '▁골', '롬', '-', '딕', '맨', '▁상수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상수', '같은', '▁상', '수는', '▁다른', '▁수학', '상수', '▁또는', '▁함수', '와', '▁약한', '▁상관', '관계', '▁또는', '▁강한', '▁상관', '관계를', '▁갖는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '▁인간과', '▁사회를', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '▁하위', '분야', '이다', '.', '▁간단하게', '▁설명', '하면', ',', '▁언어를', '▁통해', '▁인간의', '▁삶을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형상', '화한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문예', '(', '文', '藝', ')', '라고', '▁부르는', '▁것이', '▁옳', '으며', ',', '▁문학을', '▁학문의', '▁대상', '으로서', '▁탐구', '하는', '▁학문의', '▁명칭', '▁역시', '▁문예', '학', '이다', '.', '▁문예', '학은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵심', '분야', '로서', '▁인문', '학의', '▁하위', '범', '주에', '▁포함된다', '.']\n",
      "['▁반영', '론적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품을', '▁창작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감상', '하는', '▁입장', '이고', ',', '▁내재', '적', '▁관', '점의', '▁감', '상은', '▁작품의', '▁형식', ',', '▁내용에', '▁국한', '하여', '▁감상', '하는', '▁것이다', '.', '▁표현', '론적', '▁관', '점의', '▁감', '상은', '▁작가의', '▁전기', '적', '▁사실과', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것이고', ',', '▁수용', '론적', '▁관', '점의', '▁감', '상은', '▁독', '자와', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라의', '▁각', '▁현황', '과', '▁주권', '▁승인', '▁정보를', '▁개', '요', '▁형태로', '▁나열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록에', '▁포함되지', '▁않은', '▁다음', '▁국가는', '▁몬테', '비', '데오', '▁협약', '의', '▁모든', '▁조건을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질의', '▁성질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그에', '▁수반', '하는', '▁에너지의', '▁변화를', '▁연구하는', '▁자연과', '학의', '▁한', '▁분야이다', '.', '▁물리학', '도', '▁역시', '▁물질을', '▁다루는', '▁학문', '이지만', ',', '▁물리학', '이', '▁원', '소와', '▁화합', '물을', '▁모두', '▁포함한', '▁물체의', '▁운동과', '▁에너지', ',', '▁열', '적', '·', '전기', '적', '·', '광', '학적', '·', '기계', '적', '▁속', '성을', '▁다루고', '▁이러한', '▁현상', '으로부터', '▁통일된', '▁이론을', '▁구축', '하려는', '▁것과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재하는', '▁물질을', '▁이용하여', '▁특정한', '▁목적에', '▁맞는', '▁새로운', '▁물질을', '▁합성', '하는', '▁길을', '▁제공하며', ',', '▁이는', '▁농작', '물의', '▁증', '산', ',', '▁질병의', '▁치료', '▁및', '▁예방', ',', '▁에너지', '▁효율', '▁증대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공한다', '.']\n",
      "['▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '물의', '▁범위가', '▁크게', '▁넓', '어져', '▁탄소', '▁사슬', '▁또는', '▁탄소', '▁고', '리를', '▁가진', '▁모든', '▁화합', '물을', '▁뜻한다', '.', '▁유기', '화', '학의', '▁오랜', '▁관심', '사는', '▁유기', '▁화합', '물의', '▁합성', '▁메커니즘', '이다', '.', '▁현대에', '▁들어서', '▁핵', '자기', '▁공명', '법과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r' , encoding='utf8') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-soccer",
   "metadata": {},
   "source": [
    "이전 스텝에서 완성했던 create_pretrain_instances()를 코퍼스에 적용할 수 있는지 몇 라인에 대해서만 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "manual-shore",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95fe6fb5658467b810bee9920a4eb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '[MASK]', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 7, 25, 26, 27, 28, 36, 37], 'mask_label': ['▁결정', '학', '▁유기', '▁등도', '▁유기', '화', '학에서', '▁탄', '소로']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '[MASK]', '[MASK]', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 9, 10, 17, 18, 26, 27, 28], 'mask_label': ['▁유기', '▁화합물', '▁분석', '에', '▁플라스틱', ',', '▁유기', '화', '학에서']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '[MASK]', '[MASK]', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 25, 26, 27, 28, 41, 59, 60], 'mask_label': ['▁있어서', '▁매우', '▁등도', '▁유기', '화', '학에서', '▁연구하는', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '[MASK]', '▁달린다', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '[MASK]', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 14, 47, 48, 49, 50, 51, 62], 'mask_label': ['▁개발되어', '▁유기', '▁방법으로', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁유기']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '[MASK]', '[MASK]', '▁유기', '[MASK]', '▁분석', '에', '[MASK]', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 6, 8, 11, 33, 34, 35, 59, 60], 'mask_label': ['▁등이', '▁개발되어', '▁화합물', '▁있어서', '▁유기', '화', '학은', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '[MASK]', '[MASK]', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [15, 16, 25, 42, 43, 44, 45, 47, 62], 'mask_label': ['▁자리잡았다', '.', '▁등도', '▁분', '과', '이다', '.', '▁유기', '▁유기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 39, 40, 52, 53, 59, 60, 62], 'mask_label': ['▁유기', '▁화합물', '▁화합', '물을', '▁동물', '로부터', '▁뜻', '하였으나', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [8, 9, 10, 25, 38, 48, 49, 57, 58], 'mask_label': ['▁화합물', '▁분석', '에', '▁등도', '▁이루어진', '▁화합', '물은', '▁화합', '물을']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '[MASK]', '[MASK]', '[MASK]', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 19, 20, 21, 22, 23, 24, 25, 61], 'mask_label': ['▁중요한', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁지금은']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 42, 43, 44, 45, 46, 48, 49], 'mask_label': ['▁자리잡았다', '.', '▁분', '과', '이다', '.', '▁원래', '▁화합', '물은']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 11, 13, 26, 27, 28, 52, 53], 'mask_label': ['▁X', '선', '▁있어서', '▁중요한', '▁유기', '화', '학에서', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '[MASK]', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 12, 13, 36, 37, 41, 48, 49], 'mask_label': ['▁결정', '학', '▁매우', '▁중요한', '▁탄', '소로', '▁연구하는', '▁화합', '물은']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r', encoding='utf8') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리 함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-nerve",
   "metadata": {},
   "source": [
    "# 🔶 make_pretrain_data() : BERT pretrain 데이터셋 생성 메소드\n",
    "\n",
    "전체 전처리 과정을 거쳐 최종적으로 만들어지는 BERT pretrain 데이터셋 생성 메소드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distributed-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\", encoding='utf8') as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\", encoding='utf8') as in_f:\n",
    "        with open(out_file, \"w\", encoding='utf8') as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-ukraine",
   "metadata": {},
   "source": [
    "이제 약 400만 라인에 해당하는 전체 코퍼스에 대해 make_pretrain_data()를 구동해 봅시다. 10여 분 가량 시간이 소요될 수 있습니다.\n",
    "최종적으로 생성된 데이터셋은 json 포맷으로 저장될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "american-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_json_path = 'C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/data/bert_pre_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "virgin-magic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208fc4bbc7124ec089e9eb64a8da45f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "neural-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862285"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-paraguay",
   "metadata": {},
   "source": [
    "데이터셋 파일을 만드는 것까지 수행되었습니다.\n",
    "\n",
    "하지만 여기서 고려해야 할 점이 있습니다. 우리가 다루어야 할 데이터셋은 사이즈가 큽니다. 만들어질 json 데이터파일의 크기가 1.4GB 정도 됩니다. 실제 BERT 학습용의 백 분의 일 사이즈 정도밖에 안 되겠지만 그럼에도 불구하고 이렇게 큰 파일을 로딩하는 함수를 만들 때는 메모리 사용량과 관련해 고려해야 할 점이 있습니다.\n",
    "\n",
    "그래서 우리는 np.memmap을 사용해서 메모리 사용량을 최소화하는 방법을 시도해 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hindu-outline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5,  1605, 27599,  5551, 14146, 15991,  8637, 27599,    13,\n",
       "             6, 25987,  2247, 15033, 27873, 14475, 27813, 27873, 28196,\n",
       "         27636, 10185, 16285,  1232, 22935, 27599,  4777, 27625,   243,\n",
       "          2780,    14,  1509, 22095,   414,   165,  1697, 28290, 27873,\n",
       "         27703, 27683,   593,    21, 29007,   399,  5540,   813,    17,\n",
       "         27599,   307, 16905,     6,     6,     6, 19041, 27718,    98,\n",
       "         27878, 15784,  2543,     6,     6,     6,     6,     6,  4578,\n",
       "         27599,     4,  4427,  1239,     6,    37, 11234,  2378,  5249,\n",
       "          9858,  3294,    13, 20590,  2386,  2163, 27596, 27671,   969,\n",
       "          8047,   173,   607,  2387,   317, 27604,  3926, 27625,  5551,\n",
       "            37, 18995,  8198,  9858,  1447,     6,     6,  5551,    37,\n",
       "            18,   451,  4267, 27599,     6,  6436,    25,  5551, 27646,\n",
       "         18205,   928,   157, 27821,    61, 27773,   530, 27604,  3372,\n",
       "           523,     6,     6,  5551,    18,   982, 13264, 27599,  5551,\n",
       "             6,     4]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,   103, 28313, 28290,     0,     0,     0,\n",
       "             0,     0,     0,   309,   337,  5771, 27616, 27603,     0,\n",
       "             0,     0,  3715, 27625,  5551,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,  1921, 27625,     0,     0,\n",
       "             0,     0,     0,     0,  4864,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  3409,   673,     0,     0,     0,     0,     0,     0,\n",
       "          5053,     0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "#[CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int64)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int64)\n",
    "# segments = np.zeros((total, n_seq), np.int64)\n",
    "# labels_nsp = np.zeros((total,), np.int64)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int64)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='readwrite', dtype=np.int32, shape=(total,n_seq))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-maple",
   "metadata": {},
   "source": [
    "만들어진 json 파일을 라인 단위로 읽어 들여 np.memmap에 로딩해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "understanding-chance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f236acd64784a96beb96ecb324510ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/862285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁태어났다', '.', '▁조지아', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '[MASK]', '▁해군에', '▁들어가', '▁전함', '·', '원자', '력', '·', '잠', '수', '함의', '▁승무', '원으로', '▁일하였다', '.', '▁1953', '년', '▁미국', '▁해군', '▁대', '위로', '▁예편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈을', '▁벌', '었다', '.', '▁그의', '▁별명이', '[MASK]', '[MASK]', '[MASK]', '▁농부', '\"', '▁(', 'P', 'ean', 'ut', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁알려졌다', '.', '[SEP]', '▁늦', '되면서', '[MASK]', '▁주', '▁상원', '▁의원', '▁선거에서', '▁낙선', '하나', '▁그', '▁선거가', '▁부정', '선거', '▁', '였', '음을', '▁입증', '하게', '▁되어', '▁당선', '되고', ',', '▁1966', '년', '▁조지아', '▁주', '▁지사', '▁선거에', '▁낙선', '하지만', '[MASK]', '[MASK]', '▁조지아', '▁주', '▁지', '사를', '▁역임했다', '.', '[MASK]', '▁되기', '▁전', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '[MASK]', '[MASK]', '▁조지아', '▁지', '사로', '▁근무했다', '.', '▁조지아', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 48, 49, 50, 57, 58, 59, 60, 61, 65, 66, 67, 95, 96, 103, 118, 119, 126], 'mask_label': ['▁후', '▁\"', '땅', '콩', '▁F', 'ar', 'mer', ')', '로', '▁1962', '년', '▁조지아', '▁1970', '년', '▁대통령이', '▁1975', '년까지', '▁주지']}\n",
      "enc_token: [5, 1605, 27599, 5551, 14146, 15991, 8637, 27599, 13, 6, 25987, 2247, 15033, 27873, 14475, 27813, 27873, 28196, 27636, 10185, 16285, 1232, 22935, 27599, 4777, 27625, 243, 2780, 14, 1509, 22095, 414, 165, 1697, 28290, 27873, 27703, 27683, 593, 21, 29007, 399, 5540, 813, 17, 27599, 307, 16905, 6, 6, 6, 19041, 27718, 98, 27878, 15784, 2543, 6, 6, 6, 6, 6, 4578, 27599, 4, 4427, 1239, 6, 37, 11234, 2378, 5249, 9858, 3294, 13, 20590, 2386, 2163, 27596, 27671, 969, 8047, 173, 607, 2387, 317, 27604, 3926, 27625, 5551, 37, 18995, 8198, 9858, 1447, 6, 6, 5551, 37, 18, 451, 4267, 27599, 6, 6436, 25, 5551, 27646, 18205, 928, 157, 27821, 61, 27773, 530, 27604, 3372, 523, 6, 6, 5551, 18, 982, 13264, 27599, 5551, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0    81     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   103 28313 28290     0     0     0     0     0     0   309   337  5771\n",
      " 27616 27603     0     0     0  3715 27625  5551     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0  1921\n",
      " 27625     0     0     0     0     0     0  4864     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  3409   673\n",
      "     0     0     0     0     0     0  5053     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1976', '년', '▁대통령', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '[MASK]', '[MASK]', '▁누르고', '▁당선되었다', '.', '▁카터', '▁대통령은', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.', '[SEP]', '▁카', '터는', '▁이집', '트와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '▁안', '와', '르', '[MASK]', '[MASK]', '▁대통령과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중동', '[MASK]', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '▁체결했다', '.', '▁그러나', '▁이것은', '▁공화', '당과', '▁미국의', '▁유대인', '[MASK]', '▁반발을', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '▁양국', '▁간의', '▁평화', '조약', '으로', '[MASK]', '[MASK]', '[MASK]', '▁또한', '▁소련과', '▁제', '2', '차', '▁전략', '▁무기', '[MASK]', '▁협', '상에', '▁조인', '했다', '.', '▁카', '터는', '[MASK]', '[MASK]', '[MASK]', '▁당시', '[MASK]', '▁등', '▁인권', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '▁노력', '했으며', ',', '[MASK]', '▁이후', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [13, 14, 45, 46, 51, 52, 57, 72, 73, 85, 86, 87, 95, 103, 104, 105, 107, 120], 'mask_label': ['▁포', '드를', '▁사다', '트', '▁베', '긴', '▁평화를', '▁단체의', '▁반발을', '▁이끌', '어졌다', '.', '▁제한', '▁1970', '년대', '▁후반', '▁대한민국', '▁취임']}\n",
      "enc_token: [5, 3306, 27625, 663, 8198, 4867, 4896, 19160, 6244, 238, 22033, 19990, 27604, 6, 6, 10071, 7965, 27599, 25250, 5906, 3634, 8085, 9747, 1003, 4460, 1547, 4771, 18474, 27599, 4, 207, 4612, 2703, 3604, 3426, 27607, 3358, 54, 27604, 10251, 3640, 3552, 172, 27665, 27699, 6, 6, 13799, 334, 27637, 29887, 271, 28099, 1011, 27644, 280, 8021, 6, 521, 10251, 4282, 27694, 27681, 15990, 19102, 27599, 330, 1487, 4460, 4040, 679, 7455, 6, 21408, 6564, 27599, 2995, 27625, 10312, 6749, 13195, 2714, 2793, 8993, 9, 6, 6, 6, 276, 23197, 30, 27619, 27751, 2835, 3841, 6, 617, 1824, 15876, 31, 27599, 207, 4612, 6, 6, 6, 316, 6, 50, 5636, 17092, 137, 18896, 42, 917, 15177, 231, 3375, 530, 27604, 6, 165, 6357, 6244, 27642, 1233, 5890, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   119  1486     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  7025 27677     0\n",
      "     0     0     0   271 28099     0     0     0     0 14237     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 15747 21408     0     0     0     0     0     0     0     0     0     0\n",
      "     0  1435  2521 27599     0     0     0     0     0     0     0  1956\n",
      "     0     0     0     0     0     0     0  1921   596  1840     0   410\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  2659     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주한미', '군을', '▁철수', '하기로', '▁했다', '.', '▁그러나', '▁주한미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '[MASK]', '[MASK]', '▁부딪', '혀', '▁주한미', '군은', '▁완전', '철', '수', '▁대신', '▁6,000', '명을', '▁감축', '하는', '▁데', '▁그쳤다', '.', '▁또한', '▁박정희', '▁정권의', '▁인권', '▁문제', '▁등', '과의', '▁논란', '으로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국을', '▁방문하여', '▁관계가', '▁다소', '▁회복', '되었다', '.', '[SEP]', '▁그러나', '[MASK]', '▁이란', '▁미국', '▁대사관', '[MASK]', '[MASK]', '▁사건에서', '▁인', '질', '[MASK]', '▁실패를', '▁이유로', '[MASK]', '[MASK]', '▁대통령', '▁선거에서', '▁공화', '당의', '▁로', '널드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패했다', '.', '▁또한', '▁임기', '[MASK]', '▁터', '진', '▁소련의', '▁아프가니스탄', '▁침공', '▁사건으로', '▁인해', '▁1980', '년', '[MASK]', '▁올림픽에', '▁반공', '국가', '들의', '[MASK]', '[MASK]', '[MASK]', '▁내세', '웠다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [24, 25, 50, 51, 52, 53, 73, 74, 77, 78, 82, 85, 86, 106, 116, 121, 122, 123], 'mask_label': ['▁반', '대에', '▁불', '협', '화', '음을', '▁주', '▁이란', '▁인', '질', '▁구출', '▁1980', '년', '▁말기에', '▁하계', '▁보이', '콧', '을']}\n",
      "enc_token: [5, 27604, 2760, 673, 49, 27737, 1949, 1633, 24438, 1262, 5337, 2390, 345, 27599, 330, 24438, 27722, 3069, 2576, 1071, 1468, 27873, 27601, 511, 6, 6, 11574, 28178, 24438, 941, 4626, 27917, 27636, 1083, 23453, 859, 26346, 38, 189, 11330, 27599, 276, 5298, 13574, 5636, 550, 50, 786, 2408, 9, 6, 6, 6, 6, 15139, 191, 27604, 2995, 27625, 125, 27662, 27, 27946, 27604, 16187, 14905, 4857, 5421, 3332, 43, 27599, 4, 330, 6, 3290, 243, 18590, 6, 6, 23937, 42, 27892, 6, 22684, 1827, 6, 6, 663, 5249, 4460, 1547, 194, 8631, 1169, 27803, 958, 113, 27596, 27944, 875, 174, 2087, 9510, 27599, 276, 11034, 6, 870, 27713, 5569, 7676, 3232, 6322, 751, 1640, 27625, 6, 5825, 13948, 4398, 247, 6, 6, 6, 5890, 1853, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   141   867     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0   128 27993 27683   969     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0    37  3290     0     0    42 27892     0     0     0 11560     0\n",
      "     0  1640 27625     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0 12145     0\n",
      "     0     0     0     0     0     0     0     0  2219     0     0     0\n",
      "     0  3052 28805 27607     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁박정희', '▁대통령이', '▁기묘', '韶', '▁중앙정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '▁대해', '▁그는', '▁이', '▁사건으로', '▁큰', '▁충격을', '▁받았으며', ',', '[MASK]', '[MASK]', '▁밴', '스', '▁국무', '장', '관을', '▁조', '문사', '절로', '▁파견했다', '.', '▁12', '·', '12', '[MASK]', '▁반란', '과', '▁5.', '17', '▁쿠데타', '에', '[MASK]', '▁초기에는', '▁강하게', '▁비난', '했으나', ',', '▁미국', '[MASK]', '▁신군', '부를', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁있었고', '▁결국', '▁묵', '인', '하는', '[MASK]', '▁태도를', '▁보이게', '▁됐다', '.', '[SEP]', '▁퇴임', '▁이후', '▁민간', '▁자원을', '▁적극', '▁활용한', '▁비영리', '▁기구', '인', '▁카터', '▁재', '단을', '▁설립한', '[MASK]', '[MASK]', '▁실현', '을', '▁위해', '▁제', '▁3', '세계의', '[MASK]', '[MASK]', '▁활동', '▁및', '▁기니', '▁벌', '레', '에', '▁의한', '▁드라', '쿤', '쿠르', '스', '▁질병', '▁방', '재를', '▁장안', '▁힘썼다', '.', '▁미국의', '▁빈곤', '층', '▁지원', '▁활동', ',', '▁사랑의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분쟁', '▁중재', '▁등의', '▁활동도', '▁했다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 21, 22, 36, 43, 50, 53, 54, 55, 56, 62, 77, 81, 82, 89, 90, 105], 'mask_label': ['▁김재', '규', '▁사이', '러스', '▁군사', '▁대해', '▁정부가', '▁설득', '하는데', ',', '▁한계가', '▁듯한', '▁카터', '▁뒤', '▁민주주의', '▁선거', '▁감시', '▁위해']}\n",
      "enc_token: [5, 27604, 5298, 4864, 27387, 31931, 18525, 27638, 1312, 355, 2591, 27711, 2057, 433, 202, 8, 6322, 459, 10688, 5325, 27604, 6, 6, 1228, 27626, 4444, 27651, 1657, 53, 27181, 17544, 26520, 27599, 196, 27873, 1335, 6, 2342, 27644, 11262, 1695, 14078, 27600, 6, 6797, 7015, 3560, 1003, 27604, 243, 6, 26826, 1191, 6, 6, 6, 6, 2492, 875, 5374, 27628, 38, 6, 11162, 16915, 3842, 27599, 4, 13826, 165, 3174, 17304, 2929, 20639, 16068, 6673, 27628, 25250, 174, 1574, 7301, 6, 6, 5031, 27607, 231, 30, 49, 21655, 6, 6, 375, 228, 18137, 813, 27740, 27600, 1332, 17378, 28956, 16453, 27626, 5225, 95, 4445, 15108, 19607, 27599, 679, 14197, 28083, 770, 375, 27604, 14003, 313, 28333, 27614, 887, 27604, 605, 4476, 13267, 507, 27328, 345, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0  9918 27958     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   328  2086     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  1250     0     0     0     0     0     0   433     0     0     0     0\n",
      "     0     0  3840     0     0  5523  1294 27604 20984     0     0     0\n",
      "     0     0 10180     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0 25250     0     0     0   339  9889     0\n",
      "     0     0     0     0     0   822  7049     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   231     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁카터', '▁행정부', '▁이후', '▁미국이', '▁북', '핵', '▁위기', ',', '▁코소보', '▁전쟁', ',', '▁이라크', '▁전쟁과', '▁같이', '▁미국이', '[MASK]', '[MASK]', '▁최후', '로', '▁선택하는', '▁전통적', '▁사고를', '▁버리고', '▁나이에', '▁행동을', '▁선행', '하는', '▁행위에', '▁대해', '▁깊은', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사적', '▁활동에', '▁강한', '▁반대', '▁입장을', '▁보이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '▁분쟁', '[MASK]', '▁위해', '▁북한의', '▁김일성', ',', '▁아이', '티의', '▁세', '드', '라스', '▁장군', ',', '▁팔', '레인', '스타', '인의', '[MASK]', '[MASK]', '[MASK]', '▁보스', '니아의', '▁세르비아', '계', '▁정권', '▁같이', '▁미국', '▁정부에', '▁대해', '▁협상을', '▁거부', '하면서', '▁사태', '의', '▁위기를', '▁초래', '한', '▁인물', '▁및', '▁단체를', '▁직접', '[MASK]', '▁분쟁', '의', '[MASK]', '▁근본적으로', '▁해결하기', '▁위해', '▁힘썼다', '.', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈등을', '[MASK]', '▁했지만', ',', '▁전직', '▁대통령의', '▁권한', '과', '[MASK]', '[MASK]', '[MASK]', '▁인사', '들의', '[MASK]', '[MASK]', '[MASK]', '▁나갔다', '.', '▁1978', '년에', '▁채', '결', '된', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 18, 19, 26, 50, 66, 67, 68, 90, 93, 105, 112, 113, 114, 117, 118, 119], 'mask_label': ['▁카', '터는', '▁군사적', '▁행동을', '▁군사적', '▁조정을', '▁하', '마스', ',', '▁만나', '▁원인을', '▁보이기도', '▁재', '야', '▁유명', '▁활약으로', '▁해결', '해']}\n",
      "enc_token: [5, 6, 6, 25250, 21862, 165, 8424, 251, 28166, 10622, 27604, 26202, 506, 27604, 6157, 17305, 733, 8424, 6, 6, 12241, 27603, 26028, 15644, 13098, 8275, 7441, 5616, 16374, 38, 19690, 433, 4508, 46, 2196, 2466, 1368, 679, 9641, 8253, 2632, 1216, 5168, 7010, 28, 27599, 4, 698, 605, 4476, 6, 231, 9305, 11444, 27604, 520, 10694, 74, 27681, 1951, 4379, 27604, 961, 5346, 936, 692, 6, 6, 6, 6076, 4174, 4543, 27704, 5752, 733, 243, 6633, 433, 12149, 2324, 421, 4597, 27601, 11239, 8200, 27612, 1178, 228, 19762, 1069, 6, 4476, 27601, 6, 24806, 13425, 231, 19607, 27599, 8, 2208, 243, 895, 2576, 12627, 6, 3379, 27604, 5605, 5744, 3753, 27644, 6, 6, 6, 3329, 247, 6, 6, 6, 9946, 27599, 3331, 169, 481, 27783, 27711, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0   207  4612     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  9641  5616     0     0     0     0\n",
      "     0     0  9641     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0 23144     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    27  3678 27604     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  2142     0     0 13635     0     0\n",
      "     0     0     0     0     0     0     0     0     0 23828     0     0\n",
      "     0     0     0     0   174 27775   939     0     0 17462  2317 27645\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁북한에', '▁대한', '▁미국의', '▁군사적', '▁행동이', '▁임', '박', '했으나', ',', '[MASK]', '▁전직', '▁대통령', '으로는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁미국과', '▁북', '[MASK]', '▁중재', '에', '▁큰', '▁기여를', '▁해', '▁위기를', '▁해결', '했다는', '[MASK]', '▁받았다', '.', '▁또한', '▁이', '▁때', '[MASK]', '▁대통령과', '맏', '▁주', '석의', '▁만남', '을', '▁주선', '했다', '.', '▁하지만', '▁그로부터', '▁수', '주일', '▁후', '▁김일', '성이', '▁갑자기', '▁사망하여', '▁김일', '성과', '▁김영삼', '의', '▁정상회담', '은', '▁이루어지지', '▁못했다', '.', '[SEP]', '▁미국의', '▁관', '타나', '모', '▁수용', '소', '▁문제', ',', '▁세계의', '▁인권', '문제', '에서도', '▁관심이', '▁깊', '어', '▁유엔', '에', '▁유엔', '인권', '고등', '판', '무', '관의', '▁제도를', '▁시행', '하도록', '▁노력', '하여', '▁독재', '자들의', '▁인권', '▁유', '린', '에', '▁대해', '▁제', '약을', '▁하고', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁만드는', '▁데', '▁기여', '하여', '▁독재', '자들', '▁같은', '▁인권', '유', '린', '범죄', '자를', '▁재판', '소로', '▁회', '부', '하여', '▁국제적인', '▁처벌을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [10, 14, 15, 16, 17, 18, 21, 30, 31, 32, 35, 36, 38, 65, 104, 105, 106, 107], 'mask_label': ['▁미국', '▁처음으로', '▁북한', '을', '▁방문', '하고', '▁양국의', '▁평가를', '▁받았다', '.', '▁때', '▁김영삼', '▁김일성', '▁미국의', '▁국제', '형사', '재판', '소를']}\n",
      "enc_token: [5, 25086, 92, 679, 9641, 18507, 273, 27914, 1003, 27604, 6, 5605, 663, 1030, 6, 6, 6, 6, 6, 5672, 251, 6, 13267, 27600, 459, 13856, 87, 11239, 2317, 2351, 6, 772, 27599, 276, 8, 84, 6, 13799, 29164, 37, 5361, 11842, 27607, 25754, 31, 27599, 589, 14313, 19, 10106, 81, 4636, 684, 5908, 26809, 4636, 1693, 9133, 27601, 27509, 27613, 13475, 2041, 27599, 4, 679, 88, 8011, 27716, 2237, 27688, 550, 27604, 5467, 5636, 5515, 643, 8181, 1910, 27633, 3708, 27600, 3708, 12972, 5059, 27841, 27725, 2429, 6520, 2404, 1816, 3375, 54, 7559, 2653, 5636, 46, 27870, 27600, 433, 30, 3297, 644, 27604, 6, 6, 6, 6, 3002, 189, 2187, 54, 7559, 3989, 226, 5636, 27690, 27870, 8822, 690, 2474, 2661, 270, 27638, 54, 12835, 19956, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0   243     0\n",
      "     0     0  1307  1876 27607  2017    48     0     0 25764     0     0\n",
      "     0     0     0     0     0     0  4549   772 27599     0     0    84\n",
      "  9133     0 11444     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0   679     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0   605 17905  4731  1358\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\", encoding='utf8') as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int64)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int64)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int64, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-lemon",
   "metadata": {},
   "source": [
    "# 🔶 load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수\n",
    "\n",
    "np.memmap을 사용해 메모리 효율적으로 만들어진 데이터를 로딩하는 함수를 아래와 같이 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prostate-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "     # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='readwrite', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='readwrite', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    \n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "#     enc_tokens = np.zeros((total, n_seq), np.int64)\n",
    "#     dec_tokens = np.zeros((total, n_seq), np.int64)\n",
    "#     segments = np.zeros((total, n_seq), np.int64)\n",
    "#     labels_nsp = np.zeros((total,), np.int64)\n",
    "#     labels_mlm = np.zeros((total, n_seq), np.int64)\n",
    "\n",
    "    with open(filename, \"r\" , encoding='utf8') as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int64)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int64)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int64, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hazardous-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848400ac85dc4c4aabce3476592fe697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "perfect-belize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5,  1605, 27599,  5551, 14146, 15991,  8637, 27599,    13,\n",
       "             6, 25987,  2247, 15033, 27873, 14475, 27813, 27873, 28196,\n",
       "         27636, 10185, 16285,  1232, 22935, 27599,  4777, 27625,   243,\n",
       "          2780,    14,  1509, 22095,   414,   165,  1697, 28290, 27873,\n",
       "         27703, 27683,   593,    21, 29007,   399,  5540,   813,    17,\n",
       "         27599,   307, 16905,     6,     6,     6, 19041, 27718,    98,\n",
       "         27878, 15784,  2543,     6,     6,     6,     6,     6,  4578,\n",
       "         27599,     4,  4427,  1239,     6,    37, 11234,  2378,  5249,\n",
       "          9858,  3294,    13, 20590,  2386,  2163, 27596, 27671,   969,\n",
       "          8047,   173,   607,  2387,   317, 27604,  3926, 27625,  5551,\n",
       "            37, 18995,  8198,  9858,  1447,     6,     6,  5551,    37,\n",
       "            18,   451,  4267, 27599,     6,  6436,    25,  5551, 27646,\n",
       "         18205,   928,   157, 27821,    61, 27773,   530, 27604,  3372,\n",
       "           523,     6,     6,  5551,    18,   982, 13264, 27599,  5551,\n",
       "             6,     4]),\n",
       " memmap([    5,  2191, 27599,  5078,    81, 27604,   342,  4812, 27625,\n",
       "           294, 14456,     6, 24930,  2540, 27600,   488,  4871,  2524,\n",
       "         13358,   171, 27599,   330, 27604, 14456,     6,  4842, 27682,\n",
       "         27625,  2131, 14135,  9943,   761, 28254,   658,   171, 27599,\n",
       "            13,     6, 13069, 11312,   496,  8020,  1910, 27633,  4216,\n",
       "           664,   926,  5238,  1053,  5583, 27614,  2419,  2470, 27604,\n",
       "         21622, 27602,   838, 15403, 27760,   824,  3525, 13998,  7619,\n",
       "         27599,     4,   371, 27604,    29, 28018, 27793,  1326, 27787,\n",
       "            66,   412, 22289,    28, 27599,  1326,  5884,     6,  2573,\n",
       "            66,  1599, 27653,   639,  3883,   353, 27599, 17506,  5263,\n",
       "          1326,  5884,    19,   402,     6,     6,     6,     6,     6,\n",
       "          2742,    31, 27599,     6, 16343,     6,     6,     6,     6,\n",
       "             6,     6,     6,     6, 10603, 27616,     9, 25545,  1599,\n",
       "         27653,   639,   254,   238,   787,  2654,   784, 27604,  1925,\n",
       "           259,     4]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 1,\n",
       " 0,\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,   103, 28313, 28290,     0,     0,     0,\n",
       "             0,     0,     0,   309,   337,  5771, 27616, 27603,     0,\n",
       "             0,     0,  3715, 27625,  5551,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,  1921, 27625,     0,     0,\n",
       "             0,     0,     0,     0,  4864,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  3409,   673,     0,     0,     0,     0,     0,     0,\n",
       "          5053,     0]),\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,    89,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 15170,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  4031,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 11497,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,   231, 11497,  9933, 27607,  2042,\n",
       "             0,     0,     0, 11364,     0,  1911, 27604,    68, 27650,\n",
       "         27617,  3065, 28116, 27601,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-productivity",
   "metadata": {},
   "source": [
    "# 14-6. BERT 모델 구현"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "acquired-resort",
   "metadata": {},
   "source": [
    "이제 본격적으로 BERT model을 구현해 보겠습니다.\n",
    "\n",
    "BERT가 transformer encoder로 구현되어 있다는 것은 잘 알고 계시리라 생각합니다. 이미 여러 번 다뤄보셨을 transformer의 모델 구조와 거의 유사하지만, 아래 그림과 같이 3개의 embedding 레이어를 가진다는 점에 유의해야 합니다.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "우선 몇 가지 유틸리티 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "central-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "perfect-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fleet-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "common-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "supported-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eligible-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-style",
   "metadata": {},
   "source": [
    "이제 본격적으로 embedding 레이어를 쌓아나가겠습니다. 아래는 Token Embedding의 구현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "desirable-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-degree",
   "metadata": {},
   "source": [
    "Positional Embedding 레이어는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "informal-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-bracket",
   "metadata": {},
   "source": [
    "상대적으로 매우 간단한 Segment Embedding은 별도의 레이어를 구현하지 않고 BERT 클래스에서 간단히 포함하도록 하겠습니다.\n",
    "\n",
    "아래는 자주 보았던 ScaleDotProductAttention과 이를 활용한 MultiHeadAttention입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "specific-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "proprietary-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-nevada",
   "metadata": {},
   "source": [
    "이를 바탕으로 transformer encoder 레이어를 구성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "assigned-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "indonesian-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-recipient",
   "metadata": {},
   "source": [
    "이제 다 왔습니다.\n",
    "\n",
    "최종적으로 구성할 BERT 레이어는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "horizontal-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-portuguese",
   "metadata": {},
   "source": [
    "BERT 레이어를 바탕으로 최종적으로 만들어질 pretrain용 BERT 모델 구성은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sharp-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adaptive-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-electronics",
   "metadata": {},
   "source": [
    "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "anticipated-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "superb-personality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 3s 74ms/step - loss: 11.2227 - nsp_loss: 0.7743 - mlm_loss: 10.4484 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 10.0172 - nsp_loss: 0.6251 - mlm_loss: 9.3921 - nsp_acc: 0.8000 - mlm_acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x236b7058c70>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-burke",
   "metadata": {},
   "source": [
    "test_model.fit()이 잘 구동되나요?\n",
    "\n",
    "다음 스텝에서 본격적으로 학습을 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-genetics",
   "metadata": {},
   "source": [
    "# 14-7. pretrain 진행\n",
    "\n",
    "loss나 accuracy같이 기본적으로 필요한 계산 함수를 미리 정의해 둡시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "scientific-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "outstanding-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-stream",
   "metadata": {},
   "source": [
    "Learning Rate 스케줄링도 아래와 같이 구현합니다. WarmUp 이후 consine 형태로 감소하는 스케줄을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "determined-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "coastal-tobacco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-kansas",
   "metadata": {},
   "source": [
    "이제 모델을 실제로 빌드 해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "generic-template",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " enc_tokens (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " segments (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bert (BERT)                    ((None, 256),        10629632    ['enc_tokens[0][0]',             \n",
      "                                 (None, None, 32007               'segments[0][0]']               \n",
      "                                ))                                                                \n",
      "                                                                                                  \n",
      " pooled_nsp (PooledOutput)      (None, 2)            66304       ['bert[0][0]']                   \n",
      "                                                                                                  \n",
      " nsp (Softmax)                  (None, 2)            0           ['pooled_nsp[0][0]']             \n",
      "                                                                                                  \n",
      " mlm (Softmax)                  (None, None, 32007)  0           ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-shoot",
   "metadata": {},
   "source": [
    "이제 본격적으로 학습을 진행합니다. 1Epoch만 학습하는 데도 10분 이상의 상당한 시간이 소요될 것입니다. 그리고 메모리 오류가 날 수 있으니 배치 사이즈에도 유의해 주세요. 참고로 우리는 전체 데이터셋 중의 1/7 수준인 128000건만 로딩해서 사용 중이라는 것을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "hybrid-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 6000\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "twenty-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 21.9236 - nsp_loss: 0.6399 - mlm_loss: 21.2837 - nsp_acc: 0.6034 - mlm_lm_acc: 0.0992\n",
      "Epoch 1: mlm_lm_acc improved from -inf to 0.09917, saving model to C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/models\\bert_pretrain.hdf5\n",
      "2000/2000 [==============================] - 237s 117ms/step - loss: 21.9236 - nsp_loss: 0.6399 - mlm_loss: 21.2837 - nsp_acc: 0.6034 - mlm_lm_acc: 0.0992\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 19.9143 - nsp_loss: 0.6063 - mlm_loss: 19.3081 - nsp_acc: 0.6563 - mlm_lm_acc: 0.1283\n",
      "Epoch 2: mlm_lm_acc improved from 0.09917 to 0.12832, saving model to C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/models\\bert_pretrain.hdf5\n",
      "2000/2000 [==============================] - 236s 118ms/step - loss: 19.9143 - nsp_loss: 0.6063 - mlm_loss: 19.3081 - nsp_acc: 0.6563 - mlm_lm_acc: 0.1283\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 19.2789 - nsp_loss: 0.5700 - mlm_loss: 18.7088 - nsp_acc: 0.7145 - mlm_lm_acc: 0.1365\n",
      "Epoch 3: mlm_lm_acc improved from 0.12832 to 0.13646, saving model to C:/Users/Noah/aiffel/GoingDeeper/AIFFEL_GOINGDEEPER_NLP/G-14/models\\bert_pretrain.hdf5\n",
      "2000/2000 [==============================] - 236s 118ms/step - loss: 19.2789 - nsp_loss: 0.5700 - mlm_loss: 18.7088 - nsp_acc: 0.7145 - mlm_lm_acc: 0.1365\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pretrain.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "better-geneva",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "infrared-wireless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 작업 소요 시간은 약 990초입니다.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "work_time = round(end_time - start_time)\n",
    "print(f'총 작업 소요 시간은 약 {work_time}초입니다.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
